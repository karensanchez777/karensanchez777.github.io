[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "karensanchez777.github.io",
    "section": "",
    "text": "Creando un clasificador con 102 Category Flower Dataset\n\n\n\n\n\nEntrenemos juntos un clasificador de flores usando el conjunto de datos ‘102 Category Flower Dataset’ y la librería de fastai.\n\n\n\n\n\n\nJun 21, 2023\n\n\nKaren Sánchez\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst experiment\n\n\n\n\n\nPost description\n\n\n\n\n\n\nMay 21, 2023\n\n\nKaren Sánchez\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "",
    "text": "Nota: Para ejecutar las celdas de código de esta notebook, puedes hacerlo en Kaggle seleccionando la opción “Copy & Edit”.\nEste es mi miniproyecto de las lecciones 1 y 2 del curso Practical Deep Learning for Coders part 1.\nEn este curso se enfatiza que para sacar el máximo provecho de este tenemos que llevar a cabo miniproyectos en cada lección, ya que, como menciona Jeremy Howard, el profesor, “lo más importante para aprender Deep Learning es escribir código y experimentar”.\nLos miniproyectos se basan en lo que aprendimos durante la lección y los capítulos correspondientes del libro del curso. Pueden consistir en pequeñas modificaciones a la notebook principal, como probar arquitecturas diferentes a las presentadas o entrenar modelos con conjuntos de datos de nuestro interés.\nEn este miniproyecto me he basado en la notebook de la lección 1, Is it a bird?, así como en los capítulos 1 y 2 del libro. Mi objetivo es crear un clasificador de imágenes con 102 categorías de flores utilizando un conjunto de datos que encontré en la documentación de fast.ai.\nEn la primera lección del curso de fast.ai construimos un clasificador de imágenes que distingue entre imágenes de bosques y pájaros. Inspirada en dicho clasificador, decidí crear un clasificador de flores. Así, la próxima vez que vea una flor que me encante pero no sepa su nombre, podré tomarle una foto y preguntarle a mi modelo.\nEl conjunto de datos consta de más de 8000 imágenes de 102 tipos de flores comunes en el Reino Unido. La división del conjunto de datos en conjuntos de entrenamiento, validación y prueba, así como las etiquetas, se especifican en archivos de texto, como veremos a continuación.\nPara trabajar en Kaggle tenemos que actualizar la librería de fastai con -Uqq."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#el-conjunto-de-prueba",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#el-conjunto-de-prueba",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "El conjunto de prueba",
    "text": "El conjunto de prueba\nPor ahora, podemos prescindir del conjunto de prueba y solo apoyarnos del conjunto de validación para evaluar el desempeño del modelo. De hecho, el conjunto de prueba debe permanecer oculto para nosotros, los que estamos entrenando el modelo, y solo emplearse hasta que estemos satisfechos con los resultados que arroja nuestro modelo. No antes. El propósito del conjunto de prueba es verificar que la persona encargada de entrenar un modelo no haya cometido un sobreajuste en el conjunto de validación. Al igual que el modelo puede sobreajustarse al conjunto de entrenamiento, nosotros (los humanos) también podemos cometer sobreajuste en el conjunto de validación al tratar, de todas las formas posibles, de incrementar la precisión en dicho conjunto.\nPor tanto, no hay problema si eliminamos este conjunto por ahora. Mi motivo para hacer esto es que más adelante uso la función get_image_files, a la cual debo pasar como argumento la ubicación del directorio jpg, para indicarle a fastai donde se encuentran los datos para entrenar y validar el modelo, pero si los datos del conjunto de prueba se encuentran en el directorio jpg, estos serán considerados datos de entrenamiento, lo cual sería erróneo. Sin embargo, si no deseamos eliminarlos podemos simplemente mover los archivos a un nuevo directorio, por ejemplo, con:\n# Crear un nuevo directorio \n\ndest = path/'test_images'\ndest.mkdir(exist_ok=True, parents=True)\n\n# Mover los archivos de prueba al nuevo directorio\nfor filename in test_df['filename']:\n    source_file=path/filename\n    !mv $source_file $dest\nAdvertencia: Este bloque de código tarda aproximadamente 0.5 h en ejecutarse en mi computadora.\nHe optado por eliminar las imágenes del conjunto de prueba en lugar de moverlos a un nuevo directorio, porque la primera opción es mucho más rápida.\nNota: El inconveniente de tener que eliminar o mover los datos del conjunto de prueba surge debido al uso de la función get_image_files al crear elDataBlock. Sin embargo, existen otras formas de indicarle a fastai cómo obtener los datos de entrenamiento y validación que no requieren implementar cambios en el directorio jpg. Puedes observar un ejemplo de estas alternativas aquí, donde se utiliza el mismo conjunto de datos que yo he usado.\n\n# Ocupamos este modulo para usar la clase Path, la función `doc`, etc\nfrom fastai.vision.all import *\n\ndeleted=list(map(Path.unlink, test_df['filename'].apply(lambda x: path/x)))\nprint(f\"El número total de archivos eliminados del directorio jpg es {len(deleted)}\")\nprint(f\"Ahora el directorio jpg solo cuenta con {len(os.listdir(path/'jpg'))} imágenes\")\n\nEl número total de archivos eliminados del directorio jpg es 6149\nAhora el directorio jpg solo cuenta con 2040 imágenes\n\n\nVemos que en el directorio jpg nos hemos quedado con las imágenes de entrenamiento y validación."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#qué-es-un-datablock-y-que-son-los-dataloaders",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#qué-es-un-datablock-y-que-son-los-dataloaders",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "¿Qué es un DataBlock y que son los dataloaders?",
    "text": "¿Qué es un DataBlock y que son los dataloaders?\nComo Jeremy Howard menciona en la lección 1, “Si hay algo que debe preocuparnos como novatos es aprender a usar los DataBlocks. Ni entender qué es una red neuronal es tan importante en la práctica”. Lo primero se refiere a que sin datos no podemos entrenar un modelo. Lo segundo se refiere a que en la actualidad la comunidad de Deep Learning cuenta con modelos que pueden resolver casi cualquier tarea que nos imaginemos, es decir, no es necesario entrenar una red neuronal desde cero. Ojo: En este curso nos dicen qué es más importante en la práctica, por algo se llama PRACTICAL Deep Learning for Coders.\nEl DataBlock es una plantilla que tenemos que rellenar y sirve no solo para problemas de Computer Vision, sino que también funciona para otro tipo de tareas. Diferentes tipos de tareas comparten elementos comunes, como especificar los datos de entrada y la variable objetivo, la ubicación de los datos, cómo asignar etiquetas y cómo crear nuestro conjunto de validación. El único requisito especial para Computer Visión es que todas las imágenes deben tener el mismo tamaño.\nSin embargo, un DataBlock no hace nada hasta que no definimos los dataloaders, que es donde tendremos guardados los datos listos para comenzar el entrenamiento del modelo.\nAntes de crear nuestro DataBlock y nuestros dataloaders me gustaría hablar de las funciones que he utilizado en los parámetros del DataBlock, que, como veremos más adelante, está definido de la siguiente manera:\nflowers=DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y= get_flower_name,\n    splitter=FuncSplitter(is_valid),\n    item_tfms=Resize(460))\nLo primero que le debemos de decir a fastai el tipo de problema que estamos resolviendo, en este caso, clasificación de imagenes, por tanto, el primer parametro del DataBlock es: ## Inputs y outputs del modelo blocks=(ImageBlock, CategoryBlock): En esta parte indicamos cuál es el input y cuál es output. En este caso, por tratarse de clasificación de imágenes, cuando mi modelo haga predicciones, el input que le voy a dar es una imagen y lo que el modelo me tiene que regresar es la etiqueta o categoría de esta imagen. ## ¿Dónde están los datos? get_items=get_image_files: Aquí le decimos a fastai donde se encuentran los datos, donde hemos utilizado la función get_image_files, que conviene que revisemos un poco qué hace.\n\nget_image_files(path/'jpg')\n\n(#2040) [Path('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_07694.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_07564.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_01324.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_07225.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_04721.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_04855.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_06682.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_00719.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_04952.jpg')...]\n\n\nNota: Observemos que le he dado como argumento la dirección donde se encuentran las imágenes, mientras que en mi DataBlock no lo hice. Esto es porque la fuente de los datos la especifico cuando creo los dataloaders, después de crear el DataBlock.\nget_image_files extrae las direcciones de todas las imágenes en el folder jpg de forma recursiva, esto es, incluye las de las imágenes en los subfolders, si es que hay. Esta es la razón por la que tuvimos que eliminar las imágenes del conjunto de prueba de este directorio, ya que también serían utilizadas como datos para entrenar nuestro modelo.\nLo que esta función nos proporciona como items son las direcciones de todas las imagenes que tenemos en el directorio jpg (2040 en total).\n\ntype(Path('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg'))\n\npathlib.PosixPath\n\n\nObsevamos que el tipo de objeto de cada item es un objeto archivo, pathlib.PosixPath. Este tipo de objetos tienen la propiedad .name, que nos da el último componente de una dirección, esto es, el nombre del archivo. Por ejemplo:\n\nPath('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg').name\n\n'image_05046.jpg'\n\n\nDe aquí en adelante, recordemos que los items son objetos pathlib.PosixPath, ya que nuestras definiciones para las funciones que dividen el conjunto de datos y que etiquetan las imágenes consideran esto, en concreto la propiedad .name de estos."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#conjunto-de-validación",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#conjunto-de-validación",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Conjunto de validación",
    "text": "Conjunto de validación\n\ndef is_valid(x): return valid['filename'].str.contains(x.name).any()\n\nLa función is_valid se usa para determinar qué items pertencen al conjunto de validación. Recibe como input un objeto archivo. Revisa si en las cadenas de la columna filename del dataframe valid, hay una subcadena que coincide con el nombre del archivo, x.name, por lo que sus resultados son True o False.\nsplitter=FuncSplitter(is_valid): Llama a la función is_valid y se encarga dividir el conjunto de datos en base a los resultados deis_valid. Si es True, entonces el item se envía a los dataloaders del conjunto de validación. De otra manera, el item se envía a los dataloaders del conjunto de entrenamiento."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#etiquetas",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#etiquetas",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Etiquetas",
    "text": "Etiquetas\nLo bueno de computer vision es que podemos ver que tan buenas son las predicciones tan solo con ver la imagen y su predicción, por lo tanto sería de mayor utilidad para hacer esta inspección si tenemos como etiquetas los nombres de las flores, en lugar de etiquetas codificadas.\n\n# Source: https://www.kaggle.com/datasets/hobaak/oxford-102-flower-name-index\n\nflowers_name=pd.read_csv('/kaggle/input/oxford-102-flower-name-index/oxford_flower_102_name.csv')\nname_dict = flowers_name.set_index('Index').to_dict()['Name']\nprint(name_dict)\n\n{0: 'pink primrose', 1: 'hard-leaved pocket orchid', 2: 'canterbury bells', 3: 'sweet pea', 4: 'english marigold', 5: 'tiger lily', 6: 'moon orchid', 7: 'bird of paradise', 8: 'monkshood', 9: 'globe thistle', 10: 'snapdragon', 11: \"colt's foot\", 12: 'king protea', 13: 'spear thistle', 14: 'yellow iris', 15: 'globe-flower', 16: 'purple coneflower', 17: 'peruvian lily', 18: 'balloon flower', 19: 'giant white arum lily', 20: 'fire lily', 21: 'pincushion flower', 22: 'fritillary', 23: 'red ginger', 24: 'grape hyacinth', 25: 'corn poppy', 26: 'prince of wales feathers', 27: 'stemless gentian', 28: 'artichoke', 29: 'sweet william', 30: 'carnation', 31: 'garden phlox', 32: 'love in the mist', 33: 'mexican aster', 34: 'alpine sea holly', 35: 'ruby-lipped cattleya', 36: 'cape flower', 37: 'great masterwort', 38: 'siam tulip', 39: 'lenten rose', 40: 'barbeton daisy', 41: 'daffodil', 42: 'sword lily', 43: 'poinsettia', 44: 'bolero deep blue', 45: 'wallflower', 46: 'marigold', 47: 'buttercup', 48: 'oxeye daisy', 49: 'common dandelion', 50: 'petunia', 51: 'wild pansy', 52: 'primula', 53: 'sunflower', 54: 'pelargonium', 55: 'bishop of llandaff', 56: 'gaura', 57: 'geranium', 58: 'orange dahlia', 59: 'pink-yellow dahlia', 60: 'cautleya spicata', 61: 'japanese anemone', 62: 'black-eyed susan', 63: 'silverbush', 64: 'californian poppy', 65: 'osteospermum', 66: 'spring crocus', 67: 'bearded iris', 68: 'windflower', 69: 'tree poppy', 70: 'gazania', 71: 'azalea', 72: 'water lily', 73: 'rose', 74: 'thorn apple', 75: 'morning glory', 76: 'passion flower', 77: 'lotus lotus', 78: 'toad lily', 79: 'anthurium', 80: 'frangipani', 81: 'clematis', 82: 'hibiscus', 83: 'columbine', 84: 'desert-rose', 85: 'tree mallow', 86: 'magnolia', 87: 'cyclamen', 88: 'watercress', 89: 'canna lily', 90: 'hippeastrum', 91: 'bee balm', 92: 'ball moss', 93: 'foxglove', 94: 'bougainvillea', 95: 'camellia', 96: 'mallow', 97: 'mexican petunia', 98: 'bromelia', 99: 'blanket flower', 100: 'trumpet creeper', 101: 'blackberry lily'}\n\n\n\ntrain['flower_name']=train['label'].map(name_dict)\nvalid['flower_name']=valid['label'].map(name_dict)\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      filename\n      label\n      flower_name\n    \n  \n  \n    \n      0\n      jpg/image_03860.jpg\n      16\n      purple coneflower\n    \n    \n      1\n      jpg/image_06092.jpg\n      13\n      spear thistle\n    \n    \n      2\n      jpg/image_02400.jpg\n      42\n      sword lily\n    \n    \n      3\n      jpg/image_02852.jpg\n      55\n      bishop of llandaff\n    \n    \n      4\n      jpg/image_07710.jpg\n      96\n      mallow\n    \n  \n\n\n\n\n\nvalid.head()\n\n\n\n\n\n  \n    \n      \n      filename\n      label\n      flower_name\n    \n  \n  \n    \n      0\n      jpg/image_04467.jpg\n      89\n      canna lily\n    \n    \n      1\n      jpg/image_07129.jpg\n      44\n      bolero deep blue\n    \n    \n      2\n      jpg/image_05166.jpg\n      4\n      english marigold\n    \n    \n      3\n      jpg/image_07002.jpg\n      34\n      alpine sea holly\n    \n    \n      4\n      jpg/image_02007.jpg\n      79\n      anthurium\n    \n  \n\n\n\n\n\nimport pathlib\n\n#Función que extrae las etiquetas de objetos tipo PosixPath.\ndef get_flower_name(o):\n    assert isinstance(o, pathlib.PosixPath)\n    if valid['filename'].str.contains(o.name).any():\n        return valid[valid['filename'].str.contains(o.name)].flower_name.iloc[0] \n    else: \n        return train[train['filename'].str.contains(o.name)].flower_name.iloc[0] \n\nassert isinstance(o, pathlib.PosixPath) lo único que hace es corroborar que el input es un objeto pathlib.PosixPath.\nget_y= get_flower_name: Le podemos pasar cualquier función a get_y que obtenga la etiqueta de cada item. Por ejemplo, en esta función primero verificamos a qué conjunto de datos pertenece el item, para lo cual buscamos si el nombre del archivo (o.name) es una subcadena de las cadenas en filename del dataframevalid o train. Una vez que sabemos en qué dataframe se encuentra la información de dicho item, lo ubicamos en el dataframe correspondiente y obtenemos el valor de la flower_name para este item.\n\nget_flower_name(Path('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg'))\n\n'orange dahlia'\n\n\nComprobamos que esta función devuelve etiquetas, para lo cual le dimos un ejemplo del tipo de items que reciibirá cuando se creen los dataloaders."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#las-imágenes-deben-tener-el-mismo-tamaño",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#las-imágenes-deben-tener-el-mismo-tamaño",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Las imágenes deben tener el mismo tamaño",
    "text": "Las imágenes deben tener el mismo tamaño\nAl entrenar nuestro modelo con una GPU, la computadora no ve imagen por imagen, sino que ve un montón de imágenes a la vez, a lo que se conoce como batch. Este grupo de imágenes se almacena en un arreglo, llamado tensor, por lo que cada imagen debe tener el mismo tamaño.\nitem_tfms = Resize(460): item_tfms son item transforms, una operación o pedazo de código que se aplica a cada elemento o item. Un ejemplo de estos transforms es Resize.\nResize(460) Ajusta el tamaño de las imágenes al tamaño de 460 píxeles . El método por default que usa para ajustar el tamaño es crop, que recortar la imagen en forma de cuadro del tamaño especificado. Otros métodos para ajustar el tamaño de una imagen son squish y pad, para probarlos sustituimos Resize(460) por Resize(460, ResizeMethod.Squish) y Resize(128, ResizeMethod.Pad,pad_mode='zeros'), respectivamente.\n\n¿Por qué 460 píxeles?\nPodemos elegir el tamaño que deseemos pero tenemos que considerar que: * Con imágenes grandes, el modelo puede capturar más detalles de la imagen, por ende, el modelo tendrá mejores resultados. Sin embargo, las desventajas son que se requiere más memoria y recursos computacionales, lo que puede resultar en una menor velocidad de entrenamiento.\n\nCon imágenes pequeñas, se reducen los detalles de las misma, pero requieren menos memoria y recursos, lo que puede acelerar el entrenamiento.\n\n\nflowers=DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y= get_flower_name,\n    splitter=FuncSplitter(is_valid),\n    item_tfms=Resize(460))\n\ndls=flowers.dataloaders(path/'jpg', bs=32) \n\nEl argumento que le damos a dataloaders es la fuente donde se encuentran las imágenes y bs=32, que significa que el número de imágenes que contiene un batch es 32.\nPara asegurarnos que nuestros dataloaders se formaron correctamente podemos verificar el número de datos que hay en cada dataloader, con esto además verificamos si nuestra función para dividir el conjunto de datos trabaja de acuerdo a lo esperado.\n\nprint(f\"El número de batches en el dataloader de validación es: {len(dls.valid)}\")\n\nEl número de batches en el dataloader de validación es: 32\n\n\nEste resultado es el número de batches que hay en el dataloader de validación. Recordemos que en cada batch tenemos 32 imágenes (bs=32). Por lo tanto, en el conjunto de validación tenemos 32x32=1024 datos.\n\nprint(f\"El número de batches en el conjunto de entrenamiento es: {len(dls.train)}\")\n\nEl número de batches en el conjunto de entrenamiento es: 31\n\n\nEn cambio, en el conjunto de entrenamiento tenemos 31x32=992 datos.\nPodemos ver que los conjuntos no contienen cada uno las 1020 imágenes que se supone debe haber en cada conjunto. El número de imágenes usadas fue: 992+1024=2016 de 2040 imágenes disponibles, por ende, se están desperdiciando 24 imágenes. Esto es producto del tamaño que elegimos para los batch, bs=32, ya que 1020 (número de imagenes destinadas para cada dataloader) no es múltiplo de 32. Debido a que sin excepción, cada batch debe estar formado con 32 imágenes, fastai hizo lo mejor que pudo para tener batchs de ese tamaño y desperdiciar el menor número de imagenes posibles, pues si en ambos dataloaders hubiesen 32 batches, rebasaríamos el número total de imágenes y si ambos dataloaders tuvieran 31 batches, se estarían desperdiciendo más imagenes de las que están desperdiciando ahora.\nRespecto a las imágenes que no están siendo utilizadas me pregunto cómo podremos saber a qué categoría pertenecen, si es una imagen por categoría o si se han quitado todas las imágenes de más de dos categorías (recordemos que cada categoría se entrena con 10 imágenes). Esto es importante porque debido a esta disminución de datos el modelo podría tener poca confianza en las predicciones de algunas categorías.\nConsejo: Tenemos que elegir un tamaño de batch que emplee la mayoría, sino todas, las imágenes. En mi caso, funcionaría cualquier divisor de 1020 (por ejemplo, 30).\nTambién podemos inspeccionar que nuestra función para etiquetar imágenes y que el diccionario que mapea los índices con el nombre de las flores están trabajando correctamente mostrando un batch, como sigue:\n\ndls.show_batch(max_n=16)\n\n\n\n\nCon una búsqueda rapida en la web podemos verificar que las flores que se muestran arriba coinciden con el nombre que se les da. Por lo que alcance a ver, todo marcha bien."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Modelos",
    "text": "Modelos\nEn general, en Machine Learning un modelo es igual a una arquitectura más un conjunto de parámetros entrenados. Un modelo, cuando termina de entrenarse, es lo mismo que los programas típicos de computadora, ya que recibe un input (por ejemplo, una imagen) y nos devuelve un resultado (la etiqueta predicha). Por ejemplo, resnet18 es un modelo cuya arquitectura o función matemática es una red neuronal, mientras que sus parametros entrenados son los que se obtuvieron de entrenar en el conjunto de datos ImageNet, toda está información puede consultarse aquí."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#métrica",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#métrica",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Métrica",
    "text": "Métrica\nmetrics=error_rate. La métrica mide la calidad de las predicciones de un modelo usando un conjunto de validación. error_rate nos da el porcentaje de imágenes en el conjunto de validación que fueron etiquetadas incorrectamente."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos-preentrenados",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos-preentrenados",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Modelos preentrenados",
    "text": "Modelos preentrenados\nSon modelos que han sido entrenados en conjuntos de datos enormes, como ImageNet, que contiene más de 1 millón de imágenes de alrededor de 1000 categorías diferentes. Podemos aprovechar el conocimiento general que han adquirido, por ejemplo, para modelos preentrenados en imágenes, podemos beneficiarnos de su capacidad para reconocer patrones y características visuales en tareas similares a aquellas para las que fueron originalmente entrenados. Esto nos ahorra el tiempo que tomaría entrenar una red neuronal desde cero. Además, otra gran ventaja es que para personalizar el modelo preentrenado para nuestro problema no se requiere una cantidad tan grande de datos. Por ejemplo, en Is it bird? se usaron ¡30 imágenes para cada categoría!\nEl modelo preentrenado que estamos usando, resnet18, fue entrenado para resolver una tarea similiar, aunque no exactamente igual a la nuestra, ya que ambas implican clasificación de imágenes. Dado que el modelo preentrenado fue entrenado para realizar una tarea diferente a la que estamos abordando, necesitamos aplicar una técnica conocida como transfer learning para adaptar el modelo preentrenado a nuestra tarea específica.\n\nlearn.fine_tune(15)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      5.644185\n      2.874754\n      0.652941\n      00:25\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      3.068787\n      2.352395\n      0.528431\n      00:22\n    \n    \n      1\n      2.514914\n      1.718902\n      0.372549\n      00:21\n    \n    \n      2\n      1.851037\n      1.191888\n      0.253922\n      00:22\n    \n    \n      3\n      1.279319\n      0.913255\n      0.201961\n      00:22\n    \n    \n      4\n      0.841372\n      0.735211\n      0.152941\n      00:22\n    \n    \n      5\n      0.554581\n      0.648134\n      0.140196\n      00:21\n    \n    \n      6\n      0.365206\n      0.595047\n      0.138235\n      00:22\n    \n    \n      7\n      0.244178\n      0.561940\n      0.124510\n      00:22\n    \n    \n      8\n      0.166631\n      0.528423\n      0.122549\n      00:21\n    \n    \n      9\n      0.114712\n      0.517312\n      0.120588\n      00:22\n    \n    \n      10\n      0.087335\n      0.492507\n      0.111765\n      00:22\n    \n    \n      11\n      0.063537\n      0.481973\n      0.109804\n      00:21\n    \n    \n      12\n      0.056369\n      0.477516\n      0.112745\n      00:21\n    \n    \n      13\n      0.047275\n      0.475519\n      0.115686\n      00:22\n    \n    \n      14\n      0.043560\n      0.477489\n      0.114706\n      00:21\n    \n  \n\n\n\nfine_tune: Su argumento es el número de epochs, esto es, el número de veces que el modelo ve nuestros datos. Daremos más detalles sobre esta función más adelante.\nCuando usamos un modelo preentrenado, tenemos que cortar las últimas capas de la red neuronal de dicho modelo, porque en dichas capas se encuentra el conocimiento específico de la tarea para la que originalmente fue entrenado. Cuando menciono “conocimiento específico”, me refiero a los elementos únicos que distinguen al objeto objetivo de nuestra tarea. Por ejemplo, si nuestra tarea es reconocer perros en imágenes, los elementos únicos podrían ser la naríz, el hocico, las patas y la silueta del perro. Por otro lado, si estamos tratando de distinguir imágenes de flores, los elementos únicos podrían ser los tallos, las hojas, los pétalos y su densidad. Sin embargo, para que la red neuronal llegue a reconocer estas figuras complejas comienza por aprender, en las capas iniciales de la red neuronal, los elementos generales de cualquier imagen, como rectas, círculos, esquinas, gradientes de colores, y poco a poco va conectando estos elementos, conforme van aumentando las capas de una red neuronal, hasta reconocer figuras complejas, como la forma del hocico de un perro o la estructura de los petalos de un flor. El libro ofrece una excelente visualización de lo que una red neuronal de 5 capas aprende en cada capa, en la subsección “What Our Image Recognizer Learned” del capítulo 1.\nDespués de cortar las capas finales del modelo preentrenado se añaden nuevas capas a la red, con pesos aleatorios, y el número de capas dependerá del tamaño de nuestro conjunto de datos. Estas nuevas capas se denominan cabeza del modelo.\nPara adaptar el modelo preentrenado a nuestra tarea específica, aplicamos una técnica conocida como transfer learning.\n\nTransfer Learning\nEs el proceso de usar un modelo preentrenado, por todo lo que esté ya sabe hacer, para resolver una tarea diferente a la que fue entrenado originalemente. En el caso de clasificación de imágenes, como mencionamos anteriormente, nos interesa que el modelo preentrenado tenga conocimiento de los elementos esenciales de cualquier imagen, pero no estamos interesados en el conocimiento específico de las imágenes con las que fue originalmente entrenado. Esta técnica se lleva a cabo de la siguiente manera:\n\nvision_learner se encarga de eliminar las últimas capas del modelo preentrenado y reemplazarlas por nuevas capas con pesos aleatorios. El número de capas que se añade depende del tamaño de nuestro conjunto de datos.\nLa función fine_tune, exclusiva de fastai, se encarga de adaptar nuestro conjunto de datos al modelo preentrenado:\n2.1 En el primer epoch, ajusta las partes del modelo para que la nueva cabeza aleatoria trabaje correctamente en nuestro conjunto de datos.\n2.2 Durante epochs solicitados, actualiza los pesos, especialmente los de la cabeza, mientras que los pesos de las capas iniciales casi no ocurre ningún cambio.\n\nVeamos qué tan bien lo hizo nuestro modelo con la matriz de confusión.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(15, 15))"
  },
  {
    "objectID": "posts/image-clasification/quarto-prueba.html",
    "href": "posts/image-clasification/quarto-prueba.html",
    "title": "First experiment",
    "section": "",
    "text": "This is my first experiment trying to blog using a Jupyter Notebook I’ve created in Kaggle.\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.1.2 requires requests<2.29,>=2.24.0, but you have requests 2.30.0 which is incompatible.\nlibrosa 0.10.0.post2 requires soundfile>=0.12.1, but you have soundfile 0.11.0 which is incompatible.\nkfp 1.8.20 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\nkfp 1.8.20 requires PyYAML<6,>=5.3, but you have pyyaml 6.0 which is incompatible.\ngcsfs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2023.4.0 which is incompatible.\nbeatrix-jupyterlab 2023.46.184821 requires jupyter-server~=1.16, but you have jupyter-server 2.5.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\n\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=10):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nfrom fastdownload import download_url\n# Ocupamos el siguiente modulo para abrir las imagenes con `Image.open` usar la clase Path, la función `doc`, etc\nfrom fastai.vision.all import *\n\ndownload_url(search_images('rosa flor', max_images=1)[0], 'rosa.jpg', show_progress=False)\nImage.open('rosa.jpg').to_thumb(256,256)\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\nSearching for 'rosa flor'\n\n\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\n\ndownload_url(search_images('flor de dalia', max_images=1)[0], 'dalia.jpg', show_progress=False)\nImage.open('dalia.jpg').to_thumb(256,256)\n\nSearching for 'flor de dalia'\n\n\n\n\n\n\ndownload_url(search_images('sunflower flower', max_images=1)[0], 'sunflower.jpg', show_progress=False)\nImage.open('sunflower.jpg').to_thumb(256,256)\n\nSearching for 'sunflower flower'\n\n\n\n\n\n\ndownload_url(search_images('pink primrose', max_images=1)[0], 'pink_primrose.jpg', show_progress=False)\nImage.open('pink_primrose.jpg').to_thumb(256,256)\n\nSearching for 'pink primrose'\n\n\n\n\n\n\nsearches = 'sunflower flower','pink primrose flower' # Karen: búsquedas de imagenes tipo Google Images\npath = Path('flowers')\nfrom time import sleep\n\nfor o in searches: # Karen: la 'o' se va a iterar, primero toma la cadena 'forest' y después la cadena 'bird'\n    dest = (path/o)# Karen: ¿qué significa path/o? La direccion donde lo vamos a guardar. El signo '/' no es para una división, sino que es la diagonal utilizada para especificar la direccion de la computadora donde está guardado un archivo.\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o}')) \n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'sunflower flower'\nSearching for 'pink primrose flower'\n\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n0\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), #Karen: ImagenBlock=input, CategoryBlock=output\n    get_items=get_image_files, # Karen: los datos de entrenemiento; get_image_files es una función\n    splitter=RandomSplitter(valid_pct=0.2, seed=42), #Karen: separa los datos en un conjunto de entrenamiento y validación\n    get_y=parent_label, #Karen: y es la variable objetivo, este dato lo toma del nombre del folder en que está cada imagen\n    item_tfms=[Resize(192, method='squish')] # Para que todas las imágenes sean del mismo tamaño (solo si para Computer Vision)\n).dataloaders(path, bs=4)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\nlen(dls.train)\n\n3\n\n\n\nlen(dls.valid)\n\n1\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate) #Karen: dls=datos, restnet18= modelo = red neuronal\nlearn.fine_tune(1) #no empezamos desde cero, empezamos de un modelo preentrenado, fastai ajusta los pesos para que el modelo aprenda a reconocer nuestro conjunto de datos\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 71.0MB/s]\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.338883\n      0.347549\n      0.333333\n      00:05\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.795316\n      0.193980\n      0.000000\n      00:01\n    \n  \n\n\n\n\nflower_name, index, probs=learn.predict('sunflower.jpg')\nprint(f'It is a {flower_name} with probability {probs[index]:.4f}')\n\n\n\n\n\n\n\n\nIt is a sunflower flower with probability 0.9705\n\n\n\nflower_name, index, probs=learn.predict('pink_primrose.jpg')\nprint(f'It is a {flower_name} with probability {probs[index]:.4f}')\n\n\n\n\n\n\n\n\nIt is a pink primrose flower with probability 0.6853\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(3, nrows=2,figsize=(17,4))"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]