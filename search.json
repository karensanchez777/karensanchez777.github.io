[
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "",
    "text": "Nota: Para ejecutar las celdas de código de esta notebook, puedes hacerlo en Kaggle seleccionando la opción “Copy & Edit”.\nEste es mi miniproyecto de las lecciones 1 y 2 del curso Practical Deep Learning for Coders part 1.\nEn este curso se enfatiza que para sacar el máximo provecho de este tenemos que llevar a cabo miniproyectos en cada lección, ya que, como menciona Jeremy Howard, el profesor, “lo más importante para aprender Deep Learning es escribir código y experimentar”.\nLos miniproyectos se basan en lo que aprendimos durante la lección y los capítulos correspondientes del libro del curso. Pueden consistir en pequeñas modificaciones a la notebook principal, como probar arquitecturas diferentes a las presentadas o entrenar modelos con conjuntos de datos de nuestro interés.\nEn este miniproyecto me he basado en la notebook de la lección 1, Is it a bird?, así como en los capítulos 1 y 2 del libro. Mi objetivo es crear un clasificador de imágenes con 102 categorías de flores utilizando un conjunto de datos que encontré en la documentación de fast.ai.\nEn la primera lección del curso de fast.ai construimos un clasificador de imágenes que distingue entre imágenes de bosques y pájaros. Inspirada en dicho clasificador, decidí crear un clasificador de flores. Así, la próxima vez que vea una flor que me encante pero no sepa su nombre, podré tomarle una foto y preguntarle a mi modelo.\nEl conjunto de datos consta de más de 8000 imágenes de 102 tipos de flores comunes en el Reino Unido. La división del conjunto de datos en conjuntos de entrenamiento, validación y prueba, así como las etiquetas, se especifican en archivos de texto, como veremos a continuación.\nPara trabajar en Kaggle tenemos que actualizar la librería de fastai con -Uqq.\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai"
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#el-conjunto-de-prueba",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#el-conjunto-de-prueba",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "El conjunto de prueba",
    "text": "El conjunto de prueba\nPor ahora, podemos prescindir del conjunto de prueba y solo apoyarnos del conjunto de validación para evaluar el desempeño del modelo. De hecho, el conjunto de prueba debe permanecer oculto para nosotros, los que estamos entrenando el modelo, y solo emplearse hasta que estemos satisfechos con los resultados que arroja nuestro modelo. No antes. El propósito del conjunto de prueba es verificar que la persona encargada de entrenar un modelo no haya cometido un sobreajuste en el conjunto de validación. Al igual que el modelo puede sobreajustarse al conjunto de entrenamiento, nosotros (los humanos) también podemos cometer sobreajuste en el conjunto de validación al tratar, de todas las formas posibles, de incrementar la precisión en dicho conjunto.\nPor tanto, no hay problema si eliminamos este conjunto por ahora. Mi motivo para hacer esto es que más adelante uso la función get_image_files, a la cual debo pasar como argumento la ubicación del directorio jpg, para indicarle a fastai donde se encuentran los datos para entrenar y validar el modelo, pero si los datos del conjunto de prueba se encuentran en el directorio jpg, estos serán considerados datos de entrenamiento, lo cual sería erróneo. Sin embargo, si no deseamos eliminarlos podemos simplemente mover los archivos a un nuevo directorio, por ejemplo, con:\n# Crear un nuevo directorio \n\ndest = path/'test_images'\ndest.mkdir(exist_ok=True, parents=True)\n\n# Mover los archivos de prueba al nuevo directorio\nfor filename in test_df['filename']:\n    source_file=path/filename\n    !mv $source_file $dest\nAdvertencia: Este bloque de código tarda aproximadamente 0.5 h en ejecutarse en mi computadora.\nHe optado por eliminar las imágenes del conjunto de prueba en lugar de moverlos a un nuevo directorio, porque la primera opción es mucho más rápida.\nNota: El inconveniente de tener que eliminar o mover los datos del conjunto de prueba surge debido al uso de la función get_image_files al crear elDataBlock. Sin embargo, existen otras formas de indicarle a fastai cómo obtener los datos de entrenamiento y validación que no requieren implementar cambios en el directorio jpg. Puedes observar un ejemplo de estas alternativas aquí, donde se utiliza el mismo conjunto de datos que yo he usado.\n\n# Ocupamos este modulo para usar la clase Path, la función `doc`, etc\nfrom fastai.vision.all import *\n\ndeleted=list(map(Path.unlink, test_df['filename'].apply(lambda x: path/x)))\nprint(f\"El número total de archivos eliminados del directorio jpg es {len(deleted)}\")\nprint(f\"Ahora el directorio jpg solo cuenta con {len(os.listdir(path/'jpg'))} imágenes\")\n\nEl número total de archivos eliminados del directorio jpg es 6149\nAhora el directorio jpg solo cuenta con 2040 imágenes\n\n\nVemos que en el directorio jpg nos hemos quedado con las imágenes de entrenamiento y validación."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#qué-es-un-datablock-y-que-son-los-dataloaders",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#qué-es-un-datablock-y-que-son-los-dataloaders",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "¿Qué es un DataBlock y que son los dataloaders?",
    "text": "¿Qué es un DataBlock y que son los dataloaders?\nComo Jeremy Howard menciona en la lección 1, “Si hay algo que debe preocuparnos como novatos es aprender a usar los DataBlocks. Ni entender qué es una red neuronal es tan importante en la práctica”. Lo primero se refiere a que sin datos no podemos entrenar un modelo. Lo segundo se refiere a que en la actualidad la comunidad de Deep Learning cuenta con modelos que pueden resolver casi cualquier tarea que nos imaginemos, es decir, no es necesario entrenar una red neuronal desde cero. Ojo: En este curso nos dicen qué es más importante en la práctica, por algo se llama PRACTICAL Deep Learning for Coders.\nEl DataBlock es una plantilla que tenemos que rellenar y sirve no solo para problemas de Computer Vision, sino que también funciona para otro tipo de tareas. Diferentes tipos de tareas comparten elementos comunes, como especificar los datos de entrada y la variable objetivo, la ubicación de los datos, cómo asignar etiquetas y cómo crear nuestro conjunto de validación. El único requisito especial para Computer Visión es que todas las imágenes deben tener el mismo tamaño.\nSin embargo, un DataBlock no hace nada hasta que no definimos los dataloaders, que es donde tendremos guardados los datos listos para comenzar el entrenamiento del modelo.\nAntes de crear nuestro DataBlock y nuestros dataloaders me gustaría hablar de las funciones que he utilizado en los parámetros del DataBlock, que, como veremos más adelante, está definido de la siguiente manera:\nflowers=DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y= get_flower_name,\n    splitter=FuncSplitter(is_valid),\n    item_tfms=Resize(460))\nLo primero que le debemos de decir a fastai el tipo de problema que estamos resolviendo, en este caso, clasificación de imagenes, por tanto, el primer parametro del DataBlock es: ## Inputs y outputs del modelo blocks=(ImageBlock, CategoryBlock): En esta parte indicamos cuál es el input y cuál es output. En este caso, por tratarse de clasificación de imágenes, cuando mi modelo haga predicciones, el input que le voy a dar es una imagen y lo que el modelo me tiene que regresar es la etiqueta o categoría de esta imagen. ## ¿Dónde están los datos? get_items=get_image_files: Aquí le decimos a fastai donde se encuentran los datos, donde hemos utilizado la función get_image_files, que conviene que revisemos un poco qué hace.\n\nget_image_files(path/'jpg')\n\n(#2040) [Path('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_07694.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_07564.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_01324.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_07225.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_04721.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_04855.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_06682.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_00719.jpg'),Path('/root/.fastai/data/oxford-102-flowers/jpg/image_04952.jpg')...]\n\n\nNota: Observemos que le he dado como argumento la dirección donde se encuentran las imágenes, mientras que en mi DataBlock no lo hice. Esto es porque la fuente de los datos la especifico cuando creo los dataloaders, después de crear el DataBlock.\nget_image_files extrae las direcciones de todas las imágenes en el folder jpg de forma recursiva, esto es, incluye las de las imágenes en los subfolders, si es que hay. Esta es la razón por la que tuvimos que eliminar las imágenes del conjunto de prueba de este directorio, ya que también serían utilizadas como datos para entrenar nuestro modelo.\nLo que esta función nos proporciona como items son las direcciones de todas las imagenes que tenemos en el directorio jpg (2040 en total).\n\ntype(Path('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg'))\n\npathlib.PosixPath\n\n\nObsevamos que el tipo de objeto de cada item es un objeto archivo, pathlib.PosixPath. Este tipo de objetos tienen la propiedad .name, que nos da el último componente de una dirección, esto es, el nombre del archivo. Por ejemplo:\n\nPath('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg').name\n\n'image_05046.jpg'\n\n\nDe aquí en adelante, recordemos que los items son objetos pathlib.PosixPath, ya que nuestras definiciones para las funciones que dividen el conjunto de datos y que etiquetan las imágenes consideran esto, en concreto la propiedad .name de estos."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#conjunto-de-validación",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#conjunto-de-validación",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Conjunto de validación",
    "text": "Conjunto de validación\n\ndef is_valid(x): return valid['filename'].str.contains(x.name).any()\n\nLa función is_valid se usa para determinar qué items pertencen al conjunto de validación. Recibe como input un objeto archivo. Revisa si en las cadenas de la columna filename del dataframe valid, hay una subcadena que coincide con el nombre del archivo, x.name, por lo que sus resultados son True o False.\nsplitter=FuncSplitter(is_valid): Llama a la función is_valid y se encarga dividir el conjunto de datos en base a los resultados deis_valid. Si es True, entonces el item se envía a los dataloaders del conjunto de validación. De otra manera, el item se envía a los dataloaders del conjunto de entrenamiento."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#etiquetas",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#etiquetas",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Etiquetas",
    "text": "Etiquetas\nLo bueno de computer vision es que podemos ver que tan buenas son las predicciones tan solo con ver la imagen y su predicción, por lo tanto sería de mayor utilidad para hacer esta inspección si tenemos como etiquetas los nombres de las flores, en lugar de etiquetas codificadas.\n\n# Source: https://www.kaggle.com/datasets/hobaak/oxford-102-flower-name-index\n\nflowers_name=pd.read_csv('/kaggle/input/oxford-102-flower-name-index/oxford_flower_102_name.csv')\nname_dict = flowers_name.set_index('Index').to_dict()['Name']\nprint(name_dict)\n\n{0: 'pink primrose', 1: 'hard-leaved pocket orchid', 2: 'canterbury bells', 3: 'sweet pea', 4: 'english marigold', 5: 'tiger lily', 6: 'moon orchid', 7: 'bird of paradise', 8: 'monkshood', 9: 'globe thistle', 10: 'snapdragon', 11: \"colt's foot\", 12: 'king protea', 13: 'spear thistle', 14: 'yellow iris', 15: 'globe-flower', 16: 'purple coneflower', 17: 'peruvian lily', 18: 'balloon flower', 19: 'giant white arum lily', 20: 'fire lily', 21: 'pincushion flower', 22: 'fritillary', 23: 'red ginger', 24: 'grape hyacinth', 25: 'corn poppy', 26: 'prince of wales feathers', 27: 'stemless gentian', 28: 'artichoke', 29: 'sweet william', 30: 'carnation', 31: 'garden phlox', 32: 'love in the mist', 33: 'mexican aster', 34: 'alpine sea holly', 35: 'ruby-lipped cattleya', 36: 'cape flower', 37: 'great masterwort', 38: 'siam tulip', 39: 'lenten rose', 40: 'barbeton daisy', 41: 'daffodil', 42: 'sword lily', 43: 'poinsettia', 44: 'bolero deep blue', 45: 'wallflower', 46: 'marigold', 47: 'buttercup', 48: 'oxeye daisy', 49: 'common dandelion', 50: 'petunia', 51: 'wild pansy', 52: 'primula', 53: 'sunflower', 54: 'pelargonium', 55: 'bishop of llandaff', 56: 'gaura', 57: 'geranium', 58: 'orange dahlia', 59: 'pink-yellow dahlia', 60: 'cautleya spicata', 61: 'japanese anemone', 62: 'black-eyed susan', 63: 'silverbush', 64: 'californian poppy', 65: 'osteospermum', 66: 'spring crocus', 67: 'bearded iris', 68: 'windflower', 69: 'tree poppy', 70: 'gazania', 71: 'azalea', 72: 'water lily', 73: 'rose', 74: 'thorn apple', 75: 'morning glory', 76: 'passion flower', 77: 'lotus lotus', 78: 'toad lily', 79: 'anthurium', 80: 'frangipani', 81: 'clematis', 82: 'hibiscus', 83: 'columbine', 84: 'desert-rose', 85: 'tree mallow', 86: 'magnolia', 87: 'cyclamen', 88: 'watercress', 89: 'canna lily', 90: 'hippeastrum', 91: 'bee balm', 92: 'ball moss', 93: 'foxglove', 94: 'bougainvillea', 95: 'camellia', 96: 'mallow', 97: 'mexican petunia', 98: 'bromelia', 99: 'blanket flower', 100: 'trumpet creeper', 101: 'blackberry lily'}\n\n\n\ntrain['flower_name']=train['label'].map(name_dict)\nvalid['flower_name']=valid['label'].map(name_dict)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nfilename\nlabel\nflower_name\n\n\n\n\n0\njpg/image_03860.jpg\n16\npurple coneflower\n\n\n1\njpg/image_06092.jpg\n13\nspear thistle\n\n\n2\njpg/image_02400.jpg\n42\nsword lily\n\n\n3\njpg/image_02852.jpg\n55\nbishop of llandaff\n\n\n4\njpg/image_07710.jpg\n96\nmallow\n\n\n\n\n\n\n\n\nvalid.head()\n\n\n\n\n\n\n\n\nfilename\nlabel\nflower_name\n\n\n\n\n0\njpg/image_04467.jpg\n89\ncanna lily\n\n\n1\njpg/image_07129.jpg\n44\nbolero deep blue\n\n\n2\njpg/image_05166.jpg\n4\nenglish marigold\n\n\n3\njpg/image_07002.jpg\n34\nalpine sea holly\n\n\n4\njpg/image_02007.jpg\n79\nanthurium\n\n\n\n\n\n\n\n\nimport pathlib\n\n#Función que extrae las etiquetas de objetos tipo PosixPath.\ndef get_flower_name(o):\n    assert isinstance(o, pathlib.PosixPath)\n    if valid['filename'].str.contains(o.name).any():\n        return valid[valid['filename'].str.contains(o.name)].flower_name.iloc[0] \n    else: \n        return train[train['filename'].str.contains(o.name)].flower_name.iloc[0] \n\nassert isinstance(o, pathlib.PosixPath) lo único que hace es corroborar que el input es un objeto pathlib.PosixPath.\nget_y= get_flower_name: Le podemos pasar cualquier función a get_y que obtenga la etiqueta de cada item. Por ejemplo, en esta función primero verificamos a qué conjunto de datos pertenece el item, para lo cual buscamos si el nombre del archivo (o.name) es una subcadena de las cadenas en filename del dataframevalid o train. Una vez que sabemos en qué dataframe se encuentra la información de dicho item, lo ubicamos en el dataframe correspondiente y obtenemos el valor de la flower_name para este item.\n\nget_flower_name(Path('/root/.fastai/data/oxford-102-flowers/jpg/image_05046.jpg'))\n\n'orange dahlia'\n\n\nComprobamos que esta función devuelve etiquetas, para lo cual le dimos un ejemplo del tipo de items que reciibirá cuando se creen los dataloaders."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#las-imágenes-deben-tener-el-mismo-tamaño",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#las-imágenes-deben-tener-el-mismo-tamaño",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Las imágenes deben tener el mismo tamaño",
    "text": "Las imágenes deben tener el mismo tamaño\nAl entrenar nuestro modelo con una GPU, la computadora no ve imagen por imagen, sino que ve un montón de imágenes a la vez, a lo que se conoce como batch. Este grupo de imágenes se almacena en un arreglo, llamado tensor, por lo que cada imagen debe tener el mismo tamaño.\nitem_tfms = Resize(460): item_tfms son item transforms, una operación o pedazo de código que se aplica a cada elemento o item. Un ejemplo de estos transforms es Resize.\nResize(460) Ajusta el tamaño de las imágenes al tamaño de 460 píxeles . El método por default que usa para ajustar el tamaño es crop, que recortar la imagen en forma de cuadro del tamaño especificado. Otros métodos para ajustar el tamaño de una imagen son squish y pad, para probarlos sustituimos Resize(460) por Resize(460, ResizeMethod.Squish) y Resize(128, ResizeMethod.Pad,pad_mode='zeros'), respectivamente.\n\n¿Por qué 460 píxeles?\nPodemos elegir el tamaño que deseemos pero tenemos que considerar que: * Con imágenes grandes, el modelo puede capturar más detalles de la imagen, por ende, el modelo tendrá mejores resultados. Sin embargo, las desventajas son que se requiere más memoria y recursos computacionales, lo que puede resultar en una menor velocidad de entrenamiento.\n\nCon imágenes pequeñas, se reducen los detalles de las misma, pero requieren menos memoria y recursos, lo que puede acelerar el entrenamiento.\n\n\nflowers=DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y= get_flower_name,\n    splitter=FuncSplitter(is_valid),\n    item_tfms=Resize(460))\n\ndls=flowers.dataloaders(path/'jpg', bs=32) \n\nEl argumento que le damos a dataloaders es la fuente donde se encuentran las imágenes y bs=32, que significa que el número de imágenes que contiene un batch es 32.\nPara asegurarnos que nuestros dataloaders se formaron correctamente podemos verificar el número de datos que hay en cada dataloader, con esto además verificamos si nuestra función para dividir el conjunto de datos trabaja de acuerdo a lo esperado.\n\nprint(f\"El número de batches en el dataloader de validación es: {len(dls.valid)}\")\n\nEl número de batches en el dataloader de validación es: 32\n\n\nEste resultado es el número de batches que hay en el dataloader de validación. Recordemos que en cada batch tenemos 32 imágenes (bs=32). Por lo tanto, en el conjunto de validación tenemos 32x32=1024 datos.\n\nprint(f\"El número de batches en el conjunto de entrenamiento es: {len(dls.train)}\")\n\nEl número de batches en el conjunto de entrenamiento es: 31\n\n\nEn cambio, en el conjunto de entrenamiento tenemos 31x32=992 datos.\nPodemos ver que los conjuntos no contienen cada uno las 1020 imágenes que se supone debe haber en cada conjunto. El número de imágenes usadas fue: 992+1024=2016 de 2040 imágenes disponibles, por ende, se están desperdiciando 24 imágenes. Esto es producto del tamaño que elegimos para los batch, bs=32, ya que 1020 (número de imagenes destinadas para cada dataloader) no es múltiplo de 32. Debido a que sin excepción, cada batch debe estar formado con 32 imágenes, fastai hizo lo mejor que pudo para tener batchs de ese tamaño y desperdiciar el menor número de imagenes posibles, pues si en ambos dataloaders hubiesen 32 batches, rebasaríamos el número total de imágenes y si ambos dataloaders tuvieran 31 batches, se estarían desperdiciendo más imagenes de las que están desperdiciando ahora.\nRespecto a las imágenes que no están siendo utilizadas me pregunto cómo podremos saber a qué categoría pertenecen, si es una imagen por categoría o si se han quitado todas las imágenes de más de dos categorías (recordemos que cada categoría se entrena con 10 imágenes). Esto es importante porque debido a esta disminución de datos el modelo podría tener poca confianza en las predicciones de algunas categorías.\nConsejo: Tenemos que elegir un tamaño de batch que emplee la mayoría, sino todas, las imágenes. En mi caso, funcionaría cualquier divisor de 1020 (por ejemplo, 30).\nTambién podemos inspeccionar que nuestra función para etiquetar imágenes y que el diccionario que mapea los índices con el nombre de las flores están trabajando correctamente mostrando un batch, como sigue:\n\ndls.show_batch(max_n=16)\n\n\n\n\n\n\n\n\nCon una búsqueda rapida en la web podemos verificar que las flores que se muestran arriba coinciden con el nombre que se les da. Por lo que alcance a ver, todo marcha bien."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Modelos",
    "text": "Modelos\nEn general, en Machine Learning un modelo es igual a una arquitectura más un conjunto de parámetros entrenados. Un modelo, cuando termina de entrenarse, es lo mismo que los programas típicos de computadora, ya que recibe un input (por ejemplo, una imagen) y nos devuelve un resultado (la etiqueta predicha). Por ejemplo, resnet18 es un modelo cuya arquitectura o función matemática es una red neuronal, mientras que sus parametros entrenados son los que se obtuvieron de entrenar en el conjunto de datos ImageNet, toda está información puede consultarse aquí."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#métrica",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#métrica",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Métrica",
    "text": "Métrica\nmetrics=error_rate. La métrica mide la calidad de las predicciones de un modelo usando un conjunto de validación. error_rate nos da el porcentaje de imágenes en el conjunto de validación que fueron etiquetadas incorrectamente."
  },
  {
    "objectID": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos-preentrenados",
    "href": "posts/2023-06-21-flower-classification/Flower-Classification-with-fastai-and-Oxford102.html#modelos-preentrenados",
    "title": "Creando un clasificador con 102 Category Flower Dataset",
    "section": "Modelos preentrenados",
    "text": "Modelos preentrenados\nSon modelos que han sido entrenados en conjuntos de datos enormes, como ImageNet, que contiene más de 1 millón de imágenes de alrededor de 1000 categorías diferentes. Podemos aprovechar el conocimiento general que han adquirido, por ejemplo, para modelos preentrenados en imágenes, podemos beneficiarnos de su capacidad para reconocer patrones y características visuales en tareas similares a aquellas para las que fueron originalmente entrenados. Esto nos ahorra el tiempo que tomaría entrenar una red neuronal desde cero. Además, otra gran ventaja es que para personalizar el modelo preentrenado para nuestro problema no se requiere una cantidad tan grande de datos. Por ejemplo, en Is it bird? se usaron ¡30 imágenes para cada categoría!\nEl modelo preentrenado que estamos usando, resnet18, fue entrenado para resolver una tarea similiar, aunque no exactamente igual a la nuestra, ya que ambas implican clasificación de imágenes. Dado que el modelo preentrenado fue entrenado para realizar una tarea diferente a la que estamos abordando, necesitamos aplicar una técnica conocida como transfer learning para adaptar el modelo preentrenado a nuestra tarea específica.\n\nlearn.fine_tune(15)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n5.644185\n2.874754\n0.652941\n00:25\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n3.068787\n2.352395\n0.528431\n00:22\n\n\n1\n2.514914\n1.718902\n0.372549\n00:21\n\n\n2\n1.851037\n1.191888\n0.253922\n00:22\n\n\n3\n1.279319\n0.913255\n0.201961\n00:22\n\n\n4\n0.841372\n0.735211\n0.152941\n00:22\n\n\n5\n0.554581\n0.648134\n0.140196\n00:21\n\n\n6\n0.365206\n0.595047\n0.138235\n00:22\n\n\n7\n0.244178\n0.561940\n0.124510\n00:22\n\n\n8\n0.166631\n0.528423\n0.122549\n00:21\n\n\n9\n0.114712\n0.517312\n0.120588\n00:22\n\n\n10\n0.087335\n0.492507\n0.111765\n00:22\n\n\n11\n0.063537\n0.481973\n0.109804\n00:21\n\n\n12\n0.056369\n0.477516\n0.112745\n00:21\n\n\n13\n0.047275\n0.475519\n0.115686\n00:22\n\n\n14\n0.043560\n0.477489\n0.114706\n00:21\n\n\n\n\n\nfine_tune: Su argumento es el número de epochs, esto es, el número de veces que el modelo ve nuestros datos. Daremos más detalles sobre esta función más adelante.\nCuando usamos un modelo preentrenado, tenemos que cortar las últimas capas de la red neuronal de dicho modelo, porque en dichas capas se encuentra el conocimiento específico de la tarea para la que originalmente fue entrenado. Cuando menciono “conocimiento específico”, me refiero a los elementos únicos que distinguen al objeto objetivo de nuestra tarea. Por ejemplo, si nuestra tarea es reconocer perros en imágenes, los elementos únicos podrían ser la naríz, el hocico, las patas y la silueta del perro. Por otro lado, si estamos tratando de distinguir imágenes de flores, los elementos únicos podrían ser los tallos, las hojas, los pétalos y su densidad. Sin embargo, para que la red neuronal llegue a reconocer estas figuras complejas comienza por aprender, en las capas iniciales de la red neuronal, los elementos generales de cualquier imagen, como rectas, círculos, esquinas, gradientes de colores, y poco a poco va conectando estos elementos, conforme van aumentando las capas de una red neuronal, hasta reconocer figuras complejas, como la forma del hocico de un perro o la estructura de los petalos de un flor. El libro ofrece una excelente visualización de lo que una red neuronal de 5 capas aprende en cada capa, en la subsección “What Our Image Recognizer Learned” del capítulo 1.\nDespués de cortar las capas finales del modelo preentrenado se añaden nuevas capas a la red, con pesos aleatorios, y el número de capas dependerá del tamaño de nuestro conjunto de datos. Estas nuevas capas se denominan cabeza del modelo.\nPara adaptar el modelo preentrenado a nuestra tarea específica, aplicamos una técnica conocida como transfer learning.\n\nTransfer Learning\nEs el proceso de usar un modelo preentrenado, por todo lo que esté ya sabe hacer, para resolver una tarea diferente a la que fue entrenado originalemente. En el caso de clasificación de imágenes, como mencionamos anteriormente, nos interesa que el modelo preentrenado tenga conocimiento de los elementos esenciales de cualquier imagen, pero no estamos interesados en el conocimiento específico de las imágenes con las que fue originalmente entrenado. Esta técnica se lleva a cabo de la siguiente manera:\n\nvision_learner se encarga de eliminar las últimas capas del modelo preentrenado y reemplazarlas por nuevas capas con pesos aleatorios. El número de capas que se añade depende del tamaño de nuestro conjunto de datos.\nLa función fine_tune, exclusiva de fastai, se encarga de adaptar nuestro conjunto de datos al modelo preentrenado:\n2.1 En el primer epoch, ajusta las partes del modelo para que la nueva cabeza aleatoria trabaje correctamente en nuestro conjunto de datos.\n2.2 Durante epochs solicitados, actualiza los pesos, especialmente los de la cabeza, mientras que los pesos de las capas iniciales casi no ocurre ningún cambio.\n\nVeamos qué tan bien lo hizo nuestro modelo con la matriz de confusión.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(15, 15))"
  },
  {
    "objectID": "posts/image-clasification/quarto-prueba.html",
    "href": "posts/image-clasification/quarto-prueba.html",
    "title": "First experiment",
    "section": "",
    "text": "This is my first experiment trying to blog using a Jupyter Notebook I’ve created in Kaggle.\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.1.2 requires requests&lt;2.29,&gt;=2.24.0, but you have requests 2.30.0 which is incompatible.\nlibrosa 0.10.0.post2 requires soundfile&gt;=0.12.1, but you have soundfile 0.11.0 which is incompatible.\nkfp 1.8.20 requires google-api-python-client&lt;2,&gt;=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\nkfp 1.8.20 requires PyYAML&lt;6,&gt;=5.3, but you have pyyaml 6.0 which is incompatible.\ngcsfs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2023.4.0 which is incompatible.\nbeatrix-jupyterlab 2023.46.184821 requires jupyter-server~=1.16, but you have jupyter-server 2.5.0 which is incompatible.\napache-beam 2.46.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow&lt;10.0.0,&gt;=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\n\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=10):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nfrom fastdownload import download_url\n# Ocupamos el siguiente modulo para abrir las imagenes con `Image.open` usar la clase Path, la función `doc`, etc\nfrom fastai.vision.all import *\n\ndownload_url(search_images('rosa flor', max_images=1)[0], 'rosa.jpg', show_progress=False)\nImage.open('rosa.jpg').to_thumb(256,256)\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\nSearching for 'rosa flor'\n\n\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n\n\n\n\n\n\n\n\ndownload_url(search_images('flor de dalia', max_images=1)[0], 'dalia.jpg', show_progress=False)\nImage.open('dalia.jpg').to_thumb(256,256)\n\nSearching for 'flor de dalia'\n\n\n\n\n\n\n\n\n\n\ndownload_url(search_images('sunflower flower', max_images=1)[0], 'sunflower.jpg', show_progress=False)\nImage.open('sunflower.jpg').to_thumb(256,256)\n\nSearching for 'sunflower flower'\n\n\n\n\n\n\n\n\n\n\ndownload_url(search_images('pink primrose', max_images=1)[0], 'pink_primrose.jpg', show_progress=False)\nImage.open('pink_primrose.jpg').to_thumb(256,256)\n\nSearching for 'pink primrose'\n\n\n\n\n\n\n\n\n\n\nsearches = 'sunflower flower','pink primrose flower' # Karen: búsquedas de imagenes tipo Google Images\npath = Path('flowers')\nfrom time import sleep\n\nfor o in searches: # Karen: la 'o' se va a iterar, primero toma la cadena 'forest' y después la cadena 'bird'\n    dest = (path/o)# Karen: ¿qué significa path/o? La direccion donde lo vamos a guardar. El signo '/' no es para una división, sino que es la diagonal utilizada para especificar la direccion de la computadora donde está guardado un archivo.\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o}')) \n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'sunflower flower'\nSearching for 'pink primrose flower'\n\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n0\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), #Karen: ImagenBlock=input, CategoryBlock=output\n    get_items=get_image_files, # Karen: los datos de entrenemiento; get_image_files es una función\n    splitter=RandomSplitter(valid_pct=0.2, seed=42), #Karen: separa los datos en un conjunto de entrenamiento y validación\n    get_y=parent_label, #Karen: y es la variable objetivo, este dato lo toma del nombre del folder en que está cada imagen\n    item_tfms=[Resize(192, method='squish')] # Para que todas las imágenes sean del mismo tamaño (solo si para Computer Vision)\n).dataloaders(path, bs=4)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\n\nlen(dls.train)\n\n3\n\n\n\nlen(dls.valid)\n\n1\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate) #Karen: dls=datos, restnet18= modelo = red neuronal\nlearn.fine_tune(1) #no empezamos desde cero, empezamos de un modelo preentrenado, fastai ajusta los pesos para que el modelo aprenda a reconocer nuestro conjunto de datos\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 71.0MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.338883\n0.347549\n0.333333\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.795316\n0.193980\n0.000000\n00:01\n\n\n\n\n\n\nflower_name, index, probs=learn.predict('sunflower.jpg')\nprint(f'It is a {flower_name} with probability {probs[index]:.4f}')\n\n\n\n\n\n\n\n\nIt is a sunflower flower with probability 0.9705\n\n\n\nflower_name, index, probs=learn.predict('pink_primrose.jpg')\nprint(f'It is a {flower_name} with probability {probs[index]:.4f}')\n\n\n\n\n\n\n\n\nIt is a pink primrose flower with probability 0.6853\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(3, nrows=2,figsize=(17,4))"
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html",
    "title": "Learn how to use the Kaggle API",
    "section": "",
    "text": "Today I’ve learned how to download datasets from Kaggle using its API. Previously, I used to download datasets manually, but with the API, the process is much faster and more efficient.\nHere’s a step-by-step guide to downloading datasets using the Kaggle API, inspired by fast.ai’s Live Coding 7 session."
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-1-verify-kaggle-installation",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-1-verify-kaggle-installation",
    "title": "Learn how to use the Kaggle API",
    "section": "Step 1: Verify Kaggle Installation",
    "text": "Step 1: Verify Kaggle Installation\nFirst, check if Kaggle is installed by running the following command in your terminal:\nkaggle"
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-2-install-the-kaggle-package",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-2-install-the-kaggle-package",
    "title": "Learn how to use the Kaggle API",
    "section": "Step 2: Install the Kaggle Package",
    "text": "Step 2: Install the Kaggle Package\nIf the kaggle command is not recognized, install the package using pip:\npip install --user kaggle\nYou might see a warning similar to:\nWARNING: The script slugify is installed in '/home/user/.local/bin' which is not on PATH.\nKaggle command line tool has been installed in your home directory (because of the --user flag in the previous command), specifically in the .local/bin folder, where there are executable files. However, this folder is not included in the PATH environment variable, the place where the shell looks for executables files associated with a given command. (You can watch the part of Live Coding 3, where Jeremy Howard talks about the concept PATH in the shells.) As a result, typing kaggle again still doesn’t work out. We’ll address this issue in step number 3."
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-3-update-the-path-variable",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-3-update-the-path-variable",
    "title": "Learn how to use the Kaggle API",
    "section": "Step 3: Update the PATH Variable",
    "text": "Step 3: Update the PATH Variable\nTo resolve this warning, modify your PATH variable as follows:\n\nOpen your .bashrc file using vim:\nvim ~/.bashrc\nNavigate to the end of the file with Shift+g.\nOpen a new line by pressing o, and it automatically places the cursor in “Insert Mode” (you should see – INSERT – at the bottom of the screen).\nAdd the following line:\nexport PATH=~/.local/bin:$PATH\nYou might want to copy the above line to your clipboard (usually with Ctrl+c) and paste it in vim with a right-click of your mouse (but it depends on your terminal emulator).\nPress Esc to exit “Insert Mode”.\nSave and exit vim with :wq, then press Enter.\nUpdate your shell with the new PATH by running:\nsource !$\nsource is used to read and execute the contents of a file in the current shell session, and !$ represents the argument of the last command (remember it was vim ~/.bashrc), so in this case !$ is the same as ~/.bashrc.\n\nNow, the kaggle command should be recognized."
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-4-add-your-kaggle-api-token",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-4-add-your-kaggle-api-token",
    "title": "Learn how to use the Kaggle API",
    "section": "Step 4: Add Your Kaggle API Token",
    "text": "Step 4: Add Your Kaggle API Token\nThe Kaggle API requires a kaggle.json file containing your credentials. Follow these steps to obtain and configure the file:\n\nVisit Kaggle, click on your profile picture and select Settings.\nScroll to the API section and click Create New Token.\nA kaggle.json file will be downloaded to your system.\n\nFor Windows users, you can locate the file in your Downloads folder, typically at /mnt/c/Users/&lt;YourUsername&gt;/Downloads\n\nCreate a .kaggle directory in your home folder:\nmkdir ~/.kaggle\nCopy the kaggle.json file to this directory:\ncp /mnt/c/Users/&lt;YourUsername&gt;/Downloads/kaggle.json ~/.kaggle\nSecure your API token by modifying its permissions:\nchmod 600 ~/.kaggle/kaggle.json"
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-5-download-a-dataset",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-5-download-a-dataset",
    "title": "Learn how to use the Kaggle API",
    "section": "Step 5: Download a Dataset",
    "text": "Step 5: Download a Dataset\nTo download a dataset, navigate to its Kaggle page and copy the download command. For example, to download the dataset for the Playground Series Season 5 Episode 1 competition:\n\nAccept the competition rules on Kaggle, if applicable.\nNavigate to the directory in the terminal where you want to save the data.\nRun the download command:\nkaggle competitions download -c playground-series-s5e1"
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-6-extract-the-dataset",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#step-6-extract-the-dataset",
    "title": "Learn how to use the Kaggle API",
    "section": "Step 6: Extract the Dataset",
    "text": "Step 6: Extract the Dataset\nThe downloaded file will be a ZIP archive. Extract it using:\nunzip -q playground-series-s5e1.zip"
  },
  {
    "objectID": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#conclusion",
    "href": "posts/2025-01-19-kaggle-api/learning-to-use-kaggle-api.html#conclusion",
    "title": "Learn how to use the Kaggle API",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s it! From now on, with the Kaggle API set up, you can download datasets effortlessly using just two commands:\nkaggle competitions download -c &lt;dataset-name&gt;\nunzip -q &lt;dataset-name&gt;.zip"
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "In this ocassion I want to share with you what I’ve learned in fast.ai’s Lesson 6 and chapter 9 of the book of the same course. Turns out, I’m learning about decision trees. Thus, I think that this competition is an excelent opportunity to put in practice my knowledge.\nLet’s begin by downloading the data. The following code block determines whether you’re working in a Kaggle notebook or locally. If you’re working locally, you’ll need to use the Kaggle API. If you’re unfamiliar with it, here’s a walkthrough on how to use the Kaggle API.\n\nimport os\nfrom pathlib import Path\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path('../input/playground-series-s5e1')\nelse:\n    path = Path('playground-series-s5e1')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nImport the module of fastai dedicated to tabular data. It already includes the pandas and numpy libraries, so we don’t need to import them.\n\nfrom fastai.tabular.all import *\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nid\ndate\ncountry\nstore\nproduct\nnum_sold\n\n\n\n\n0\n0\n2010-01-01\nCanada\nDiscount Stickers\nHolographic Goose\nNaN\n\n\n1\n1\n2010-01-01\nCanada\nDiscount Stickers\nKaggle\n973.0\n\n\n2\n2\n2010-01-01\nCanada\nDiscount Stickers\nKaggle Tiers\n906.0\n\n\n3\n3\n2010-01-01\nCanada\nDiscount Stickers\nKerneler\n423.0\n\n\n4\n4\n2010-01-01\nCanada\nDiscount Stickers\nKerneler Dark Mode\n491.0\n\n\n\n\n\n\n\n\ndf_test = pd.read_csv(path/'test.csv')\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\nid\ndate\ncountry\nstore\nproduct\n\n\n\n\n0\n230130\n2017-01-01\nCanada\nDiscount Stickers\nHolographic Goose\n\n\n1\n230131\n2017-01-01\nCanada\nDiscount Stickers\nKaggle\n\n\n2\n230132\n2017-01-01\nCanada\nDiscount Stickers\nKaggle Tiers\n\n\n3\n230133\n2017-01-01\nCanada\nDiscount Stickers\nKerneler\n\n\n4\n230134\n2017-01-01\nCanada\nDiscount Stickers\nKerneler Dark Mode\n\n\n\n\n\n\n\nFirst of all, identify the dependent variable, which is num_sold:\n\ndep_var='num_sold'\n\nThe function describe of a Pandas DataFrame provide us a brief summary of the variables in the dataframe\n\ndf.describe(include=np.number)\n\n\n\n\n\n\n\n\nid\nnum_sold\n\n\n\n\ncount\n230130.000000\n221259.000000\n\n\nmean\n115064.500000\n752.527382\n\n\nstd\n66432.953062\n690.165445\n\n\nmin\n0.000000\n5.000000\n\n\n25%\n57532.250000\n219.000000\n\n\n50%\n115064.500000\n605.000000\n\n\n75%\n172596.750000\n1114.000000\n\n\nmax\n230129.000000\n5939.000000\n\n\n\n\n\n\n\n\ndf.describe(include=['object'])\n\n\n\n\n\n\n\n\ndate\ncountry\nstore\nproduct\n\n\n\n\ncount\n230130\n230130\n230130\n230130\n\n\nunique\n2557\n6\n3\n5\n\n\ntop\n2010-01-01\nCanada\nDiscount Stickers\nHolographic Goose\n\n\nfreq\n90\n38355\n76710\n46026\n\n\n\n\n\n\n\n\n\nThis a time series problem, so it might be a good idea to know how to handles dates. In order to extract information about the date column, like, what is the period of time that covers the training set, we can use the next techniques:\n\nConvert date strings to datetime objects:\n\n\ndf[\"date\"] = df[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n\n\ndf_test[\"date\"] = df_test[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n\n\ndf.describe() can also give us a quick glance at the values of the date column:\n\n\ndf.describe(include='datetime')\n\n\n\n\n\n\n\n\ndate\n\n\n\n\ncount\n230130\n\n\nmean\n2013-07-02 00:00:00\n\n\nmin\n2010-01-01 00:00:00\n\n\n25%\n2011-10-02 00:00:00\n\n\n50%\n2013-07-02 00:00:00\n\n\n75%\n2015-04-02 00:00:00\n\n\nmax\n2016-12-31 00:00:00\n\n\n\n\n\n\n\nWe note that the training set covers the data of sticker sales from January 1, 2010 to December 31, 2016.\n\nExtract details like years or months:\n\n\ndf[\"date\"].dt.year.unique()\n\narray([2010, 2011, 2012, 2013, 2014, 2015, 2016], dtype=int32)\n\n\n\ndf[\"date\"].dt.month.unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n\n\nWe confirm that the dataset contains the sales info from (the beggining of) 2010 to (the end of) 2016. What about the test dataset?\n\ndf_test[\"date\"].dt.year.unique()\n\narray([2017, 2018, 2019], dtype=int32)\n\n\nYou can also calculate time differences or derive weekday numbers using datetime. For example:\n\ndelta = datetime(2025,1,27)-datetime(2025, 1, 20)\ndelta.days\n\n7\n\n\n\ntoday=datetime.now()\nprint(f\"You can verify that today is day number {today.weekday()} of the week and week number {today.strftime('%U')} of the year.\")\n\nYou can verify that today is day number 2 of the week and week number 03 of the year.\n\n\n\n\n\nDates can provide a wealth of information when transformed into categorical or continuous variables:\n\nadd_datepart is a helpful function of the fastai’s tabular module because it creates new features such as day of the week, week of the year, and year-end indicators based on the date column. They are important to study the relationship between the sales and, for example, weekends or holidays.\n\n\ndf=add_datepart(df,\"date\")\ndf.head()\n\n\n\n\n\n\n\n\nid\ncountry\nstore\nproduct\nnum_sold\nYear\nMonth\nWeek\nDay\nDayofweek\nDayofyear\nIs_month_end\nIs_month_start\nIs_quarter_end\nIs_quarter_start\nIs_year_end\nIs_year_start\nElapsed\n\n\n\n\n0\n0\nCanada\nDiscount Stickers\nHolographic Goose\nNaN\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n1\n1\nCanada\nDiscount Stickers\nKaggle\n973.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n2\n2\nCanada\nDiscount Stickers\nKaggle Tiers\n906.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n3\n3\nCanada\nDiscount Stickers\nKerneler\n423.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n4\n4\nCanada\nDiscount Stickers\nKerneler Dark Mode\n491.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n\n\n\n\n\n\n\n\nTo train decision trees effectively:\n\nEnsure all columns are numerical, with no missing values. The next utilities of fastai address this issues in the dataset:\n\nFillMissing: Fill missing values in the continuous features of the dataset using its median by default, but it doesn’t take into account the missing values in the dependent variable.\nCategorify: Convert categorical variables to numerical codes.\n\n\nLet’s see where do we have missing values:\n\ndf.isna().sum()\n\nid                     0\ncountry                0\nstore                  0\nproduct                0\nnum_sold            8871\nYear                   0\nMonth                  0\nWeek                   0\nDay                    0\nDayofweek              0\nDayofyear              0\nIs_month_end           0\nIs_month_start         0\nIs_quarter_end         0\nIs_quarter_start       0\nIs_year_end            0\nIs_year_start          0\nElapsed                0\ndtype: int64\n\n\nWe can observe that the single variable with missing values is the dependent variable.\nDecision trees can’t function with missing values in the dependent variable (nor in the independent variables), then we should find out a way to treat them, such as, filling them or eliminating those values, depending on whether the size of the dataset is large enough. Here I’m going to fill them with the median of the dependent variable.\n\nmedian=df[\"num_sold\"].median()\ndf[\"num_sold\"]=df[\"num_sold\"].fillna(median)\ndf.isna().sum()\n\nid                  0\ncountry             0\nstore               0\nproduct             0\nnum_sold            0\nYear                0\nMonth               0\nWeek                0\nDay                 0\nDayofweek           0\nDayofyear           0\nIs_month_end        0\nIs_month_start      0\nIs_quarter_end      0\nIs_quarter_start    0\nIs_year_end         0\nIs_year_start       0\nElapsed             0\ndtype: int64\n\n\nAs there were no missing values in the features, then we only need to categorify the categorical variables.\n\nprocs=[Categorify]\n\nBut, which are the categorical variables? fastai also provides us a quick method to distinguish between continuous and categorical variables:\n\ncont,cat=cont_cat_split(df,1,dep_var)\n\n\nprint(f\"Continuous variables: {cont}\")\nprint(f\"Categorical variables: {cat}\")\n\nContinuous variables: ['id', 'Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Elapsed']\nCategorical variables: ['country', 'store', 'product', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n\n\n\n\n\n\nFor time-series problems, remember: we are trying to predict the future.\n\nThe validation set have to represent future data. As we want the validation set to reflect the test set, we will build the validation set such that it spans the data from the last three years in the training set.\n\ncond = df[\"Year\"]&lt;2014\ntrain_idx = np.where(cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx),list(valid_idx))\n\n\nlen(splits[0])\n\n131490\n\n\n\n\n\nCreate a tabular object of fastai. We need to tell to it: * where is the data * how to process the data * which columns are continuous and wich columns are categorical * which is the target variable * and how to split the data to create training and validation sets.\n\nto = TabularPandas(df, procs, cat, cont, dep_var, splits=splits)\n\nTabularPandas returns train and valid attributes.\n\ntrain_xs, train_y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\nNow all the variables have numerical values and we are ready train the decision tree:\n\ntrain_xs.dtypes\n\ncountry                int8\nstore                  int8\nproduct                int8\nIs_month_end           int8\nIs_month_start         int8\nIs_quarter_end         int8\nIs_quarter_start       int8\nIs_year_end            int8\nIs_year_start          int8\nid                    int32\nYear                  int16\nMonth                  int8\nWeek                   int8\nDay                    int8\nDayofweek              int8\nDayofyear             int16\nElapsed             float32\ndtype: object\n\n\n\nvalid_xs.dtypes\n\ncountry                int8\nstore                  int8\nproduct                int8\nIs_month_end           int8\nIs_month_start         int8\nIs_quarter_end         int8\nIs_quarter_start       int8\nIs_year_end            int8\nIs_year_start          int8\nid                    int32\nYear                  int16\nMonth                  int8\nWeek                   int8\nDay                    int8\nDayofweek              int8\nDayofyear             int16\nElapsed             float32\ndtype: object\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(train_xs,train_y);\n\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, leaves_parallel=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m,train_xs,size=7)\n\n\n\n\n\n\n\n\nA decision tree is produced by a sequence of yes/no questions that generates binary splits. The objective of a decision tree is to locate a data point (even an unseen data point) in a group. The prediction of a decision tree is the average of the dependent variables in that group.\nIn the visualization of the decision tree above, we have: * The leaf at the top of the tree is the initial model. There lives all the data points of the training set. samples is therefore equal to the number of samples in the training set, value is the average of the dependent variable in the whole training set:\n\nvalue = train_y.mean()\nvalue\n\n774.4401\n\n\nsquared_error is the average of the squared error between true y and predicted y (value).\n\nsquared_error=((train_y - value)**2).mean()\nsquared_error\n\n504409.53\n\n\ncountry&lt;=4.5? is the first yes-no question that will create the first binary split, i.e., the first two groups. Why the decision tree algorithm decide to ask this question? The decision tree actually figure out that this was the question among all the posible yes-no questions, returns the smallest overall squared error, that is, this binary split generates the best predictions among all the possible binary splits.\n\ntrain_xs[\"country\"].unique()\n\narray([1, 2, 3, 4, 5, 6], dtype=int8)\n\n\nThe leaf at the bottom left is where the condition country&lt;=4.5 is true. We will called it “group 1”. Observe that in in group 1 the sales in countries 1,2,3 and 4, the average number of sold stickers is equal to 569.76 and there are 87660 sale records. On the other hand, for the second group, group 2, conformed by sales in countries 5 and 6, the average number of sold stickers almost doubles the number of sold stickers in group 1 and its number of sale records is aproximately the half of sale records in group 1. Then, we can say that in countries 5 and 6 the ratio of number of sold stickers per sale is significantly higher.\nThe algorithm proceed to split group 2 according to the question product&lt;=1.5?. If yes, we have group 3, where sticker 1 has an average number of sticker of 280.19 with a number of samples equals to one fifth of the data in group 2, in contrast to 1409.71 in the group 3, but with four fifths of the data in group 2. For me. it is not obvious why product 1 has been set apart from the other products, since it has the lowest num. of sold items of this split, but it migth be because there are fewer samples in this group: so my question is do they like or dislike sticker number 1?\nAnd the decision goes on making questions until it has the indicated number of leaf nodes we set in the argument of our model.\n\ntrain_xs[\"product\"].unique()\n\narray([1, 2, 3, 4, 5], dtype=int8)\n\n\n\ntrain_xs[\"store\"].unique()\n\narray([1, 3, 2], dtype=int8)\n\n\nSome considerations when training decision trees:\n\nAvoid overfitting by setting a maximum depth for the tree. Without limits, decision trees in scikit-learn can grow until only one observation remains per leaf, reducing their ability to generalize.\n\n\n# Tree without limits\nm = DecisionTreeRegressor()\nm.fit(train_xs,train_y);\n\n\n(m.get_n_leaves(),len(train_xs))\n\n(116193, 131490)\n\n\nThere are as many leaves as data points. How well is this model performing? In order to know, we need to compute the metric: mean absolute percentage error.\n\n\n\nThe Mean Absolute Percentage Error (MAPE) is given as follows: \\[\n\\text{MAPE} = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}-1}\\frac{|y_i - \\hat{y}_i|}{\\max(\\epsilon, |y_i|)}\n\\] where \\(y_i\\) is a true value, \\(\\hat{y}_i\\) is a predicted value, \\(n_{samples}\\) equals the number of samples, and \\(\\epsilon\\) is small number to ensure the formula works even if the true value is zero.\nMAPE measures the relative error between predicted and actual values. Smaller errors contribute less to the metric, making it effective for datasets with varying scales. Note that, if a true value is small, and the absolute error between true and predicted values is big, the relative error would be big.\nThe ideal value of MAPE is 0, indicating a perfect model where predicted values match the true values exactly. A MAPE value greater than 1 corresponds to an error exceeding 100% of the true value. For instance, a MAPE of 2 implies the prediction is off by 200%.\n\nfrom sklearn.metrics import mean_absolute_percentage_error\n\ndef mape(m,xs,y):\n    return mean_absolute_percentage_error(y,m.predict(xs))\n\n\nmape(m,train_xs,train_y)\n\n0.0\n\n\nHold on a second, please!\n\nmape(m,valid_xs,valid_y)\n\n0.44957757257369363\n\n\nThe model is not as good as we tought it was! The reason is that the decision tree has been overfitted since there are aproximately the same number of leaves as data points, then the decision tree is simply describing the paths (sequences of binary questions) to reach each data point in the training dataset, then it can’t generalize to unseen data.\nSo, we need to put a limit to the minimum number of data points in a leaf.\n\nm=DecisionTreeRegressor(min_samples_leaf=10)\nm.fit(train_xs,train_y)\nm.get_n_leaves(), len(train_xs)\n\n(9916, 131490)\n\n\n\nmape(m,train_xs,train_y), mape(m,valid_xs,valid_y)\n\n(0.20584061908102447, 0.385437108848875)\n\n\nNow the gap between the metric in the training set and the one in the validation set is not so large as before.\n\n\n\nApply the same data processing to the test set as the training set:\n\nCreate new features based on date column.\nIndicate which are the categorical and continuous features.\nCategorify to assign numerical codes to categorical variables.\n\nWe are going to use the decision tree with 9916 leaves:\n\ndf_test = add_datepart(df_test,\"date\")\nto_test = TabularPandas(df_test, procs, cat, cont)\n\n\ntest_xs = to_test.xs\n\n\npredictions = m.predict(test_xs)\n\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"num_sold\": predictions\n})\nsubmission.head()\n\n\n\n\n\n\n\n\nid\nnum_sold\n\n\n\n\n0\n230130\n605.000000\n\n\n1\n230131\n889.000000\n\n\n2\n230132\n822.800000\n\n\n3\n230133\n458.947368\n\n\n4\n230134\n534.388889\n\n\n\n\n\n\n\n\nsubmission.to_csv(\"submission_decision_tree.csv\",index=False)\n\n\n\n\nThe model performs poorly in the test set (test MAPE (public score) around 3.0 or 300%), thus we have to revisit:\n\nHow the validation set was created and ensure it reflects the test set.\nHow the training set was created. Maybe we need to train with more data.\nModel architecture and size. The next is to train a random forest.\nHow to handle the missing data in the dependent variable. Remember we only fill them with its median.\n\n\n\n\nHopefully, you’ve seen the usefulness of fastai’s tabular module for a quick data processing and learned a little bit about decission trees and how to train them with the scikit-learn library. See you next time!\nPS: An upvote 👍 to my Kaggle notebook will be awesome if you find it helpful 😃."
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#working-with-dates-in-pandas",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#working-with-dates-in-pandas",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "This a time series problem, so it might be a good idea to know how to handles dates. In order to extract information about the date column, like, what is the period of time that covers the training set, we can use the next techniques:\n\nConvert date strings to datetime objects:\n\n\ndf[\"date\"] = df[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n\n\ndf_test[\"date\"] = df_test[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n\n\ndf.describe() can also give us a quick glance at the values of the date column:\n\n\ndf.describe(include='datetime')\n\n\n\n\n\n\n\n\ndate\n\n\n\n\ncount\n230130\n\n\nmean\n2013-07-02 00:00:00\n\n\nmin\n2010-01-01 00:00:00\n\n\n25%\n2011-10-02 00:00:00\n\n\n50%\n2013-07-02 00:00:00\n\n\n75%\n2015-04-02 00:00:00\n\n\nmax\n2016-12-31 00:00:00\n\n\n\n\n\n\n\nWe note that the training set covers the data of sticker sales from January 1, 2010 to December 31, 2016.\n\nExtract details like years or months:\n\n\ndf[\"date\"].dt.year.unique()\n\narray([2010, 2011, 2012, 2013, 2014, 2015, 2016], dtype=int32)\n\n\n\ndf[\"date\"].dt.month.unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n\n\nWe confirm that the dataset contains the sales info from (the beggining of) 2010 to (the end of) 2016. What about the test dataset?\n\ndf_test[\"date\"].dt.year.unique()\n\narray([2017, 2018, 2019], dtype=int32)\n\n\nYou can also calculate time differences or derive weekday numbers using datetime. For example:\n\ndelta = datetime(2025,1,27)-datetime(2025, 1, 20)\ndelta.days\n\n7\n\n\n\ntoday=datetime.now()\nprint(f\"You can verify that today is day number {today.weekday()} of the week and week number {today.strftime('%U')} of the year.\")\n\nYou can verify that today is day number 2 of the week and week number 03 of the year."
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#creating-features-from-dates",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#creating-features-from-dates",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "Dates can provide a wealth of information when transformed into categorical or continuous variables:\n\nadd_datepart is a helpful function of the fastai’s tabular module because it creates new features such as day of the week, week of the year, and year-end indicators based on the date column. They are important to study the relationship between the sales and, for example, weekends or holidays.\n\n\ndf=add_datepart(df,\"date\")\ndf.head()\n\n\n\n\n\n\n\n\nid\ncountry\nstore\nproduct\nnum_sold\nYear\nMonth\nWeek\nDay\nDayofweek\nDayofyear\nIs_month_end\nIs_month_start\nIs_quarter_end\nIs_quarter_start\nIs_year_end\nIs_year_start\nElapsed\n\n\n\n\n0\n0\nCanada\nDiscount Stickers\nHolographic Goose\nNaN\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n1\n1\nCanada\nDiscount Stickers\nKaggle\n973.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n2\n2\nCanada\nDiscount Stickers\nKaggle Tiers\n906.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n3\n3\nCanada\nDiscount Stickers\nKerneler\n423.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09\n\n\n4\n4\nCanada\nDiscount Stickers\nKerneler Dark Mode\n491.0\n2010\n1\n53\n1\n4\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n1.262304e+09"
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#preparing-the-dataset",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#preparing-the-dataset",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "To train decision trees effectively:\n\nEnsure all columns are numerical, with no missing values. The next utilities of fastai address this issues in the dataset:\n\nFillMissing: Fill missing values in the continuous features of the dataset using its median by default, but it doesn’t take into account the missing values in the dependent variable.\nCategorify: Convert categorical variables to numerical codes.\n\n\nLet’s see where do we have missing values:\n\ndf.isna().sum()\n\nid                     0\ncountry                0\nstore                  0\nproduct                0\nnum_sold            8871\nYear                   0\nMonth                  0\nWeek                   0\nDay                    0\nDayofweek              0\nDayofyear              0\nIs_month_end           0\nIs_month_start         0\nIs_quarter_end         0\nIs_quarter_start       0\nIs_year_end            0\nIs_year_start          0\nElapsed                0\ndtype: int64\n\n\nWe can observe that the single variable with missing values is the dependent variable.\nDecision trees can’t function with missing values in the dependent variable (nor in the independent variables), then we should find out a way to treat them, such as, filling them or eliminating those values, depending on whether the size of the dataset is large enough. Here I’m going to fill them with the median of the dependent variable.\n\nmedian=df[\"num_sold\"].median()\ndf[\"num_sold\"]=df[\"num_sold\"].fillna(median)\ndf.isna().sum()\n\nid                  0\ncountry             0\nstore               0\nproduct             0\nnum_sold            0\nYear                0\nMonth               0\nWeek                0\nDay                 0\nDayofweek           0\nDayofyear           0\nIs_month_end        0\nIs_month_start      0\nIs_quarter_end      0\nIs_quarter_start    0\nIs_year_end         0\nIs_year_start       0\nElapsed             0\ndtype: int64\n\n\nAs there were no missing values in the features, then we only need to categorify the categorical variables.\n\nprocs=[Categorify]\n\nBut, which are the categorical variables? fastai also provides us a quick method to distinguish between continuous and categorical variables:\n\ncont,cat=cont_cat_split(df,1,dep_var)\n\n\nprint(f\"Continuous variables: {cont}\")\nprint(f\"Categorical variables: {cat}\")\n\nContinuous variables: ['id', 'Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Elapsed']\nCategorical variables: ['country', 'store', 'product', 'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']"
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#validation-set",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#validation-set",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "For time-series problems, remember: we are trying to predict the future.\n\nThe validation set have to represent future data. As we want the validation set to reflect the test set, we will build the validation set such that it spans the data from the last three years in the training set.\n\ncond = df[\"Year\"]&lt;2014\ntrain_idx = np.where(cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx),list(valid_idx))\n\n\nlen(splits[0])\n\n131490"
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#tabularpandas",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#tabularpandas",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "Create a tabular object of fastai. We need to tell to it: * where is the data * how to process the data * which columns are continuous and wich columns are categorical * which is the target variable * and how to split the data to create training and validation sets.\n\nto = TabularPandas(df, procs, cat, cont, dep_var, splits=splits)\n\nTabularPandas returns train and valid attributes.\n\ntrain_xs, train_y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\nNow all the variables have numerical values and we are ready train the decision tree:\n\ntrain_xs.dtypes\n\ncountry                int8\nstore                  int8\nproduct                int8\nIs_month_end           int8\nIs_month_start         int8\nIs_quarter_end         int8\nIs_quarter_start       int8\nIs_year_end            int8\nIs_year_start          int8\nid                    int32\nYear                  int16\nMonth                  int8\nWeek                   int8\nDay                    int8\nDayofweek              int8\nDayofyear             int16\nElapsed             float32\ndtype: object\n\n\n\nvalid_xs.dtypes\n\ncountry                int8\nstore                  int8\nproduct                int8\nIs_month_end           int8\nIs_month_start         int8\nIs_quarter_end         int8\nIs_quarter_start       int8\nIs_year_end            int8\nIs_year_start          int8\nid                    int32\nYear                  int16\nMonth                  int8\nWeek                   int8\nDay                    int8\nDayofweek              int8\nDayofyear             int16\nElapsed             float32\ndtype: object"
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#training-decision-trees",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#training-decision-trees",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(train_xs,train_y);\n\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, leaves_parallel=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m,train_xs,size=7)\n\n\n\n\n\n\n\n\nA decision tree is produced by a sequence of yes/no questions that generates binary splits. The objective of a decision tree is to locate a data point (even an unseen data point) in a group. The prediction of a decision tree is the average of the dependent variables in that group.\nIn the visualization of the decision tree above, we have: * The leaf at the top of the tree is the initial model. There lives all the data points of the training set. samples is therefore equal to the number of samples in the training set, value is the average of the dependent variable in the whole training set:\n\nvalue = train_y.mean()\nvalue\n\n774.4401\n\n\nsquared_error is the average of the squared error between true y and predicted y (value).\n\nsquared_error=((train_y - value)**2).mean()\nsquared_error\n\n504409.53\n\n\ncountry&lt;=4.5? is the first yes-no question that will create the first binary split, i.e., the first two groups. Why the decision tree algorithm decide to ask this question? The decision tree actually figure out that this was the question among all the posible yes-no questions, returns the smallest overall squared error, that is, this binary split generates the best predictions among all the possible binary splits.\n\ntrain_xs[\"country\"].unique()\n\narray([1, 2, 3, 4, 5, 6], dtype=int8)\n\n\nThe leaf at the bottom left is where the condition country&lt;=4.5 is true. We will called it “group 1”. Observe that in in group 1 the sales in countries 1,2,3 and 4, the average number of sold stickers is equal to 569.76 and there are 87660 sale records. On the other hand, for the second group, group 2, conformed by sales in countries 5 and 6, the average number of sold stickers almost doubles the number of sold stickers in group 1 and its number of sale records is aproximately the half of sale records in group 1. Then, we can say that in countries 5 and 6 the ratio of number of sold stickers per sale is significantly higher.\nThe algorithm proceed to split group 2 according to the question product&lt;=1.5?. If yes, we have group 3, where sticker 1 has an average number of sticker of 280.19 with a number of samples equals to one fifth of the data in group 2, in contrast to 1409.71 in the group 3, but with four fifths of the data in group 2. For me. it is not obvious why product 1 has been set apart from the other products, since it has the lowest num. of sold items of this split, but it migth be because there are fewer samples in this group: so my question is do they like or dislike sticker number 1?\nAnd the decision goes on making questions until it has the indicated number of leaf nodes we set in the argument of our model.\n\ntrain_xs[\"product\"].unique()\n\narray([1, 2, 3, 4, 5], dtype=int8)\n\n\n\ntrain_xs[\"store\"].unique()\n\narray([1, 3, 2], dtype=int8)\n\n\nSome considerations when training decision trees:\n\nAvoid overfitting by setting a maximum depth for the tree. Without limits, decision trees in scikit-learn can grow until only one observation remains per leaf, reducing their ability to generalize.\n\n\n# Tree without limits\nm = DecisionTreeRegressor()\nm.fit(train_xs,train_y);\n\n\n(m.get_n_leaves(),len(train_xs))\n\n(116193, 131490)\n\n\nThere are as many leaves as data points. How well is this model performing? In order to know, we need to compute the metric: mean absolute percentage error."
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#metric-and-model-evaluation",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#metric-and-model-evaluation",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "The Mean Absolute Percentage Error (MAPE) is given as follows: \\[\n\\text{MAPE} = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}-1}\\frac{|y_i - \\hat{y}_i|}{\\max(\\epsilon, |y_i|)}\n\\] where \\(y_i\\) is a true value, \\(\\hat{y}_i\\) is a predicted value, \\(n_{samples}\\) equals the number of samples, and \\(\\epsilon\\) is small number to ensure the formula works even if the true value is zero.\nMAPE measures the relative error between predicted and actual values. Smaller errors contribute less to the metric, making it effective for datasets with varying scales. Note that, if a true value is small, and the absolute error between true and predicted values is big, the relative error would be big.\nThe ideal value of MAPE is 0, indicating a perfect model where predicted values match the true values exactly. A MAPE value greater than 1 corresponds to an error exceeding 100% of the true value. For instance, a MAPE of 2 implies the prediction is off by 200%.\n\nfrom sklearn.metrics import mean_absolute_percentage_error\n\ndef mape(m,xs,y):\n    return mean_absolute_percentage_error(y,m.predict(xs))\n\n\nmape(m,train_xs,train_y)\n\n0.0\n\n\nHold on a second, please!\n\nmape(m,valid_xs,valid_y)\n\n0.44957757257369363\n\n\nThe model is not as good as we tought it was! The reason is that the decision tree has been overfitted since there are aproximately the same number of leaves as data points, then the decision tree is simply describing the paths (sequences of binary questions) to reach each data point in the training dataset, then it can’t generalize to unseen data.\nSo, we need to put a limit to the minimum number of data points in a leaf.\n\nm=DecisionTreeRegressor(min_samples_leaf=10)\nm.fit(train_xs,train_y)\nm.get_n_leaves(), len(train_xs)\n\n(9916, 131490)\n\n\n\nmape(m,train_xs,train_y), mape(m,valid_xs,valid_y)\n\n(0.20584061908102447, 0.385437108848875)\n\n\nNow the gap between the metric in the training set and the one in the validation set is not so large as before."
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#making-a-submission",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#making-a-submission",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "Apply the same data processing to the test set as the training set:\n\nCreate new features based on date column.\nIndicate which are the categorical and continuous features.\nCategorify to assign numerical codes to categorical variables.\n\nWe are going to use the decision tree with 9916 leaves:\n\ndf_test = add_datepart(df_test,\"date\")\nto_test = TabularPandas(df_test, procs, cat, cont)\n\n\ntest_xs = to_test.xs\n\n\npredictions = m.predict(test_xs)\n\n\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"num_sold\": predictions\n})\nsubmission.head()\n\n\n\n\n\n\n\n\nid\nnum_sold\n\n\n\n\n0\n230130\n605.000000\n\n\n1\n230131\n889.000000\n\n\n2\n230132\n822.800000\n\n\n3\n230133\n458.947368\n\n\n4\n230134\n534.388889\n\n\n\n\n\n\n\n\nsubmission.to_csv(\"submission_decision_tree.csv\",index=False)"
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#improving-performance",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#improving-performance",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "The model performs poorly in the test set (test MAPE (public score) around 3.0 or 300%), thus we have to revisit:\n\nHow the validation set was created and ensure it reflects the test set.\nHow the training set was created. Maybe we need to train with more data.\nModel architecture and size. The next is to train a random forest.\nHow to handle the missing data in the dependent variable. Remember we only fill them with its median."
  },
  {
    "objectID": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#final-thoughts",
    "href": "posts/2025-01-22-decision-trees/getting-started-with-decision-trees-using-fastai.html#final-thoughts",
    "title": "Getting Started with Decision Trees Using fastai",
    "section": "",
    "text": "Hopefully, you’ve seen the usefulness of fastai’s tabular module for a quick data processing and learned a little bit about decission trees and how to train them with the scikit-learn library. See you next time!\nPS: An upvote 👍 to my Kaggle notebook will be awesome if you find it helpful 😃."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "karensanchez777.github.io",
    "section": "",
    "text": "Getting Started with Decision Trees Using fastai\n\n\n\n\n\n\nTime Series\n\n\nDecision Trees\n\n\nRandom Forests\n\n\nKaggle\n\n\nfastai\n\n\nscikit-learn\n\n\n\nI show you how to use fastai’s tabular module and scikit-library to train a decision tree\n\n\n\n\n\nJan 22, 2025\n\n\nKaren Sánchez\n\n\n\n\n\n\n\n\n\n\n\n\nLearn how to use the Kaggle API\n\n\n\n\n\n\nKaggle\n\n\nAPI\n\n\nDataset\n\n\nTIL\n\n\n\n\n\n\n\n\n\nJan 19, 2025\n\n\nKaren Sánchez\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning PaddleOCR’s text detection models\n\n\n\n\n\nWe will learn how to fine-tune a PaddleOCR text detection model and to make inference with the fine-tuned model.\n\n\n\n\n\nJan 16, 2025\n\n\nKaren Sánchez\n\n\n\n\n\n\n\n\n\n\n\n\nCreando un clasificador con 102 Category Flower Dataset\n\n\n\n\n\nEntrenemos juntos un clasificador de flores usando el conjunto de datos ‘102 Category Flower Dataset’ y la librería de fastai.\n\n\n\n\n\nJun 21, 2023\n\n\nKaren Sánchez\n\n\n\n\n\n\n\n\n\n\n\n\nFirst experiment\n\n\n\n\n\nPost description\n\n\n\n\n\nMay 21, 2023\n\n\nKaren Sánchez\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html",
    "href": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html",
    "title": "Fine-tuning PaddleOCR’s text detection models",
    "section": "",
    "text": "We’ll cover the following steps to fine-tune text detection models of PaddleOCR and to make inference with our fine-tuned model.\nFirst, we need to clone the GitHub repository of PaddleOCR.\n::: {#4507debf .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2024-11-03T04:29:35.054875Z”,“iopub.status.busy”:“2024-11-03T04:29:35.054536Z”,“iopub.status.idle”:“2024-11-03T04:29:54.286380Z”,“shell.execute_reply”:“2024-11-03T04:29:54.285307Z”}’ papermill=‘{“duration”:19.242856,“end_time”:“2024-11-03T04:29:54.288700”,“exception”:false,“start_time”:“2024-11-03T04:29:35.045844”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\n:::\nNext, we have to install the gpu version of PaddlePaddle, the core deep learning framework needed to run PaddleOCR. You may want consult the PaddleOCR Quick Start.\n!python -m pip install -q paddlepaddle-gpu==2.5.0.post118 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n!pip install -q \"paddleocr&gt;=2.0.1\" # Recommend to use version 2.0.1+\n!pip install -q imutils\n!pip install -q gdown\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]&gt;=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage&gt;=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas&lt;2.1.4,&gt;=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy&lt;3.0,&gt;=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\nWe’re going to verify that our gpu paddle installation succeed with:\nimport paddle\ngpu_available  = paddle.device.is_compiled_with_cuda()\nprint(\"GPU available:\", gpu_available)\n\nGPU available: True\npaddle.utils.run_check()\n\nRunning verify PaddlePaddle program ... \n\n\nI1103 04:32:27.085585    24 interpretercore.cc:237] New Executor is Running.\nW1103 04:32:27.086083    24 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.4, Runtime API Version: 11.8\nW1103 04:32:27.096534    24 gpu_resources.cc:149] device: 0, cuDNN Version: 90.0.\n\n\nPaddlePaddle works well on 1 GPU.\n\n\nI1103 04:32:27.534034    24 interpreter_util.cc:518] Standalone Executor is Used.\n======================= Modified FLAGS detected =======================\nFLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')\n=======================================================================\nI1103 04:32:29.146459    96 tcp_utils.cc:107] Retry to connect to 127.0.0.1:55075 while the server is not yet listening.\n======================= Modified FLAGS detected =======================\nFLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')\n=======================================================================\nI1103 04:32:29.154580    95 tcp_utils.cc:181] The server starts to listen on IP_ANY:55075\nI1103 04:32:29.154767    95 tcp_utils.cc:130] Successfully connected to 127.0.0.1:55075\nI1103 04:32:32.146667    96 tcp_utils.cc:130] Successfully connected to 127.0.0.1:55075\nW1103 04:32:32.375649    95 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.4, Runtime API Version: 11.8\nW1103 04:32:32.376672    95 gpu_resources.cc:149] device: 0, cuDNN Version: 90.0.\nW1103 04:32:32.415951    96 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 7.5, Driver API Version: 12.4, Runtime API Version: 11.8\nW1103 04:32:32.416882    96 gpu_resources.cc:149] device: 1, cuDNN Version: 90.0.\nI1103 04:32:33.151151   113 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop\n\n\nPaddlePaddle works well on 2 GPUs.\nPaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\npaddle.ones([3,3])\n\nTensor(shape=[3, 3], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n       [[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\nAs we can see, our gpu version of paddle is functioning.\nIt might be useful to know that sometimes the above installation is unsuccessful, arising the following warning when running paddle.utils.run_check():\nIf that happens to you, you could find a lower version of paddlepaddle that works out for your gpu in this webpage, where you can download the whl files. Try with different versions until you find the one that is compatible with your system,as someone shows us here. It is not reccomended to use the latest version, but a stable one."
  },
  {
    "objectID": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#the-data",
    "href": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#the-data",
    "title": "Fine-tuning PaddleOCR’s text detection models",
    "section": "The data",
    "text": "The data\nThe annotations for PaddleOCR text detection models need to adhere to the next format, as indicated in the documentation, and must be .txt files, one for training and the other for evaluation:\n\" Image file name             Image annotation information encoded by json.dumps\"\nch4_test_images/img_61.jpg    [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n\npoints are the coordinates of the bounding box.\n\ntranscription is the label of the bounding box.\n\nWe’ll fine-tune a detection model on a dataset of curved text called SCUT-CTW1500, and it turns out we are lucky because the annotations with the required format are provided by PaddleOCR! You can download them here.\nNote: transcription in the dataset provided by PaddleOCR is set to 0 for all bounding box instead of containing the actual text of its bounding box, but since we’re focusing on a detection task rather than recognition, it doesn’t matter."
  },
  {
    "objectID": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#the-pretrained-model",
    "href": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#the-pretrained-model",
    "title": "Fine-tuning PaddleOCR’s text detection models",
    "section": "The pretrained model",
    "text": "The pretrained model\nPaddleOCR’s detection model supports 3 backbones (please refer to the detection models documentation): 1. MobileNetV3 2. Resnet18_vd 3. Resnet50_vd\nIt means that the model uses one of these architectures as a feature extractor, with layers trained to identify hierarchical patterns in images.\nThere are many algorithms of text detection included in PaddleOCR, for example, DBNet, SAST, EAST, etc.\nWe’ll employ the model PP-OCRv3, regarded as the best model of PaddleOCR because of its precision and generalization capabilities, as mentioned on the fine-tune guide in the docs.\n\nDownload the .tar file from this download link.\nExtract the files in the .tar file, where you’ll find student.pdparams, which will served us as our pretrained model.\n\nThe next step is to get the matching configuration file for the student model."
  },
  {
    "objectID": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#the-configuration-file",
    "href": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#the-configuration-file",
    "title": "Fine-tuning PaddleOCR’s text detection models",
    "section": "The configuration file",
    "text": "The configuration file\nThis is going to be the configuration we will utilize with our pretrained model.\n\nTXT = '''\nGlobal:\n  debug: false\n  use_gpu: true\n  epoch_num: 9\n  log_smooth_window: 20\n  print_batch_step: 10\n  save_model_dir: ./output/ch_PP-OCR_V3_det/\n  save_epoch_step: 100\n  eval_batch_step:\n  - 0\n  - 400\n  cal_metric_during_train: false\n  pretrained_model: /kaggle/input/ch-pp-ocrv3-det-distill-train/student.pdparams\n  checkpoints: null\n  save_inference_dir: null\n  use_visualdl: false\n  infer_img: doc/imgs_en/img_10.jpg\n  save_res_path: ./checkpoints/det_db/predicts_db.txt\n  distributed: true\n\nArchitecture:\n  model_type: det\n  algorithm: DB\n  Transform:\n  Backbone:\n    name: MobileNetV3\n    scale: 0.5\n    model_name: large\n    disable_se: True\n  Neck:\n    name: RSEFPN\n    out_channels: 96\n    shortcut: True\n  Head:\n    name: DBHead\n    k: 50\n\nLoss:\n  name: DBLoss\n  balance_loss: true\n  main_loss_type: DiceLoss\n  alpha: 5\n  beta: 10\n  ohem_ratio: 3\nOptimizer:\n  name: Adam\n  beta1: 0.9\n  beta2: 0.999\n  lr:\n    name: Cosine\n    learning_rate: 0.0001\n    warmup_epoch: 2\n  regularizer:\n    name: L2\n    factor: 5.0e-05\nPostProcess:\n  name: DBPostProcess\n  thresh: 0.3\n  box_thresh: 0.6\n  max_candidates: 1000\n  unclip_ratio: 1.5\nMetric:\n  name: DetMetric\n  main_indicator: hmean\nTrain:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /kaggle/input/scutctw1500-datasets/\n    label_file_list:\n      - /kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/training.txt\n    ratio_list: [1.0]\n    transforms:\n    - DecodeImage:\n        img_mode: BGR\n        channel_first: false\n    - DetLabelEncode: null\n    - IaaAugment:\n        augmenter_args:\n        - type: Fliplr\n          args:\n            p: 0.5\n        - type: Affine\n          args:\n            rotate:\n            - -10\n            - 10\n        - type: Resize\n          args:\n            size:\n            - 0.5\n            - 3\n    - EastRandomCropData:\n        size:\n        - 960\n        - 960\n        max_tries: 50\n        keep_ratio: true\n    - MakeBorderMap:\n        shrink_ratio: 0.4\n        thresh_min: 0.3\n        thresh_max: 0.7\n    - MakeShrinkMap:\n        shrink_ratio: 0.4\n        min_text_size: 8\n    - NormalizeImage:\n        scale: 1./255.\n        mean:\n        - 0.485\n        - 0.456\n        - 0.406\n        std:\n        - 0.229\n        - 0.224\n        - 0.225\n        order: hwc\n    - ToCHWImage: null\n    - KeepKeys:\n        keep_keys:\n        - image\n        - threshold_map\n        - threshold_mask\n        - shrink_map\n        - shrink_mask\n  loader:\n    shuffle: true\n    drop_last: false\n    batch_size_per_card: 8\n    num_workers: 4\nEval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /kaggle/input/scutctw1500-datasets/\n    label_file_list:\n      - /kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/test.txt\n    transforms:\n    - DecodeImage:\n        img_mode: BGR\n        channel_first: false\n    - DetLabelEncode: null\n    - DetResizeForTest: null\n    - NormalizeImage:\n        scale: 1./255.\n        mean:\n        - 0.485\n        - 0.456\n        - 0.406\n        std:\n        - 0.229\n        - 0.224\n        - 0.225\n        order: hwc\n    - ToCHWImage: null\n    - KeepKeys:\n        keep_keys:\n        - image\n        - shape\n        - polys\n        - ignore_tags\n  loader:\n    shuffle: false\n    drop_last: false\n    batch_size_per_card: 1\n    num_workers: 2\n'''\nPATH_TXT = \"/kaggle/working/config.yml\"\n\nwith open(PATH_TXT, 'w', encoding = \"utf-8\") as f:\n    f.write(TXT)\n\n\ncd /kaggle/working/\n\n/kaggle/working\n\n\n\n!cat /kaggle/working/config.yml\n\n\nGlobal:\n  debug: false\n  use_gpu: true\n  epoch_num: 9\n  log_smooth_window: 20\n  print_batch_step: 10\n  save_model_dir: ./output/ch_PP-OCR_V3_det/\n  save_epoch_step: 100\n  eval_batch_step:\n  - 0\n  - 400\n  cal_metric_during_train: false\n  pretrained_model: /kaggle/input/ch-pp-ocrv3-det-distill-train/student.pdparams\n  checkpoints: null\n  save_inference_dir: null\n  use_visualdl: false\n  infer_img: doc/imgs_en/img_10.jpg\n  save_res_path: ./checkpoints/det_db/predicts_db.txt\n  distributed: true\n\nArchitecture:\n  model_type: det\n  algorithm: DB\n  Transform:\n  Backbone:\n    name: MobileNetV3\n    scale: 0.5\n    model_name: large\n    disable_se: True\n  Neck:\n    name: RSEFPN\n    out_channels: 96\n    shortcut: True\n  Head:\n    name: DBHead\n    k: 50\n\nLoss:\n  name: DBLoss\n  balance_loss: true\n  main_loss_type: DiceLoss\n  alpha: 5\n  beta: 10\n  ohem_ratio: 3\nOptimizer:\n  name: Adam\n  beta1: 0.9\n  beta2: 0.999\n  lr:\n    name: Cosine\n    learning_rate: 0.0001\n    warmup_epoch: 2\n  regularizer:\n    name: L2\n    factor: 5.0e-05\nPostProcess:\n  name: DBPostProcess\n  thresh: 0.3\n  box_thresh: 0.6\n  max_candidates: 1000\n  unclip_ratio: 1.5\nMetric:\n  name: DetMetric\n  main_indicator: hmean\nTrain:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /kaggle/input/scutctw1500-datasets/\n    label_file_list:\n      - /kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/training.txt\n    ratio_list: [1.0]\n    transforms:\n    - DecodeImage:\n        img_mode: BGR\n        channel_first: false\n    - DetLabelEncode: null\n    - IaaAugment:\n        augmenter_args:\n        - type: Fliplr\n          args:\n            p: 0.5\n        - type: Affine\n          args:\n            rotate:\n            - -10\n            - 10\n        - type: Resize\n          args:\n            size:\n            - 0.5\n            - 3\n    - EastRandomCropData:\n        size:\n        - 960\n        - 960\n        max_tries: 50\n        keep_ratio: true\n    - MakeBorderMap:\n        shrink_ratio: 0.4\n        thresh_min: 0.3\n        thresh_max: 0.7\n    - MakeShrinkMap:\n        shrink_ratio: 0.4\n        min_text_size: 8\n    - NormalizeImage:\n        scale: 1./255.\n        mean:\n        - 0.485\n        - 0.456\n        - 0.406\n        std:\n        - 0.229\n        - 0.224\n        - 0.225\n        order: hwc\n    - ToCHWImage: null\n    - KeepKeys:\n        keep_keys:\n        - image\n        - threshold_map\n        - threshold_mask\n        - shrink_map\n        - shrink_mask\n  loader:\n    shuffle: true\n    drop_last: false\n    batch_size_per_card: 8\n    num_workers: 4\nEval:\n  dataset:\n    name: SimpleDataSet\n    data_dir: /kaggle/input/scutctw1500-datasets/\n    label_file_list:\n      - /kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/test.txt\n    transforms:\n    - DecodeImage:\n        img_mode: BGR\n        channel_first: false\n    - DetLabelEncode: null\n    - DetResizeForTest: null\n    - NormalizeImage:\n        scale: 1./255.\n        mean:\n        - 0.485\n        - 0.456\n        - 0.406\n        std:\n        - 0.229\n        - 0.224\n        - 0.225\n        order: hwc\n    - ToCHWImage: null\n    - KeepKeys:\n        keep_keys:\n        - image\n        - shape\n        - polys\n        - ignore_tags\n  loader:\n    shuffle: false\n    drop_last: false\n    batch_size_per_card: 1\n    num_workers: 2\n\n\nSome of the information that config.yml contains is:\n\nHyperparameters. The most importants are: pretrained_model, batch_size, and learning_rate.\n\nFor a single gpu:\n\nbatch_size = 8\nlearning_rate = 1e-4\n\nFor a single gpu with memory limitations:\n\nbatch_size = 4\nlearning_rate = 5e-5\n\n\nThe paths of our images and annotations.\nThe algorithm and backbone.\n\n\n!pip install pyyaml\n\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (6.0.2)\n\n\n\nimport yaml\n\n# Load the YAML file\nwith open('/kaggle/working/config.yml', 'r') as file:\n    data = yaml.safe_load(file)\n\n\nprint(f'pretrained model: {data[\"Global\"][\"pretrained_model\"]}')\nprint(f'number of epochs: {data[\"Global\"][\"epoch_num\"]}')\nprint(f'evaluate every: {data[\"Global\"][\"eval_batch_step\"][1]} iterations')\nprint(f'learning rate: {data[\"Optimizer\"][\"lr\"][\"learning_rate\"]}')\nprint(f'training batch size: {data[\"Train\"][\"loader\"][\"batch_size_per_card\"]}')\nprint(f'training images path: {data[\"Train\"][\"dataset\"][\"data_dir\"]}')\nprint(f'training annotations file path: {data[\"Train\"][\"dataset\"][\"label_file_list\"]}')\n\npretrained model: /kaggle/input/ch-pp-ocrv3-det-distill-train/student.pdparams\nnumber of epochs: 9\nevaluate every: 400 iterations\nlearning rate: 0.0001\ntraining batch size: 8\ntraining images path: /kaggle/input/scutctw1500-datasets/\ntraining annotations file path: ['/kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/training.txt']"
  },
  {
    "objectID": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#training-and-evaluation",
    "href": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#training-and-evaluation",
    "title": "Fine-tuning PaddleOCR’s text detection models",
    "section": "Training and evaluation",
    "text": "Training and evaluation\nWe’ll make use of the Kaggle’s GPU T4 x 2, so we’ll perform a multi-gpu training with the following command. For more details, refer to the documentation.\nThe model’s evaluation will be carried out every 400 iterations as was specified in config[\"Global\"][\"eval_batch_step\"].\n\nMetrics\nFor the evaluation, the detection model returns hmean as the main metric, and also computes precision and recall, as you can confirm here.\n::: {#9189533e .cell _kg_hide-input=‘false’ _kg_hide-output=‘true’ execution=‘{“iopub.execute_input”:“2024-11-03T04:32:46.567358Z”,“iopub.status.busy”:“2024-11-03T04:32:46.567063Z”,“iopub.status.idle”:“2024-11-03T04:59:08.031511Z”,“shell.execute_reply”:“2024-11-03T04:59:08.030150Z”}’ papermill=‘{“duration”:1581.485899,“end_time”:“2024-11-03T04:59:08.034245”,“exception”:false,“start_time”:“2024-11-03T04:32:46.548346”,“status”:“completed”}’ scrolled=‘true’ tags=‘[]’ execution_count=12}\n!python3 -m paddle.distributed.launch --gpus '0,1' /kaggle/working/PaddleOCR/tools/train.py -c /kaggle/working/config.yml\n\nLAUNCH INFO 2024-11-03 04:32:48,961 -----------  Configuration  ----------------------\nLAUNCH INFO 2024-11-03 04:32:48,961 auto_parallel_config: None\nLAUNCH INFO 2024-11-03 04:32:48,961 devices: 0,1\nLAUNCH INFO 2024-11-03 04:32:48,962 elastic_level: -1\nLAUNCH INFO 2024-11-03 04:32:48,962 elastic_timeout: 30\nLAUNCH INFO 2024-11-03 04:32:48,962 gloo_port: 6767\nLAUNCH INFO 2024-11-03 04:32:48,962 host: None\nLAUNCH INFO 2024-11-03 04:32:48,962 ips: None\nLAUNCH INFO 2024-11-03 04:32:48,962 job_id: default\nLAUNCH INFO 2024-11-03 04:32:48,962 legacy: False\nLAUNCH INFO 2024-11-03 04:32:48,962 log_dir: log\nLAUNCH INFO 2024-11-03 04:32:48,962 log_level: INFO\nLAUNCH INFO 2024-11-03 04:32:48,962 log_overwrite: False\nLAUNCH INFO 2024-11-03 04:32:48,962 master: None\nLAUNCH INFO 2024-11-03 04:32:48,962 max_restart: 3\nLAUNCH INFO 2024-11-03 04:32:48,962 nnodes: 1\nLAUNCH INFO 2024-11-03 04:32:48,962 nproc_per_node: None\nLAUNCH INFO 2024-11-03 04:32:48,962 rank: -1\nLAUNCH INFO 2024-11-03 04:32:48,962 run_mode: collective\nLAUNCH INFO 2024-11-03 04:32:48,962 server_num: None\nLAUNCH INFO 2024-11-03 04:32:48,962 servers: \nLAUNCH INFO 2024-11-03 04:32:48,962 start_port: 6070\nLAUNCH INFO 2024-11-03 04:32:48,963 trainer_num: None\nLAUNCH INFO 2024-11-03 04:32:48,963 trainers: \nLAUNCH INFO 2024-11-03 04:32:48,963 training_script: /kaggle/working/PaddleOCR/tools/train.py\nLAUNCH INFO 2024-11-03 04:32:48,963 training_script_args: ['-c', '/kaggle/working/config.yml']\nLAUNCH INFO 2024-11-03 04:32:48,963 with_gloo: 1\nLAUNCH INFO 2024-11-03 04:32:48,963 --------------------------------------------------\nLAUNCH INFO 2024-11-03 04:32:48,963 Job: default, mode collective, replicas 1[1:1], elastic False\nLAUNCH INFO 2024-11-03 04:32:48,965 Run Pod: hsesca, replicas 2, status ready\nLAUNCH INFO 2024-11-03 04:32:48,984 Watching Pod: hsesca, replicas 2, status running\n[2024/11/03 04:32:54] ppocr INFO: Architecture : \n[2024/11/03 04:32:54] ppocr INFO:     Backbone : \n[2024/11/03 04:32:54] ppocr INFO:         disable_se : True\n[2024/11/03 04:32:54] ppocr INFO:         model_name : large\n[2024/11/03 04:32:54] ppocr INFO:         name : MobileNetV3\n[2024/11/03 04:32:54] ppocr INFO:         scale : 0.5\n[2024/11/03 04:32:54] ppocr INFO:     Head : \n[2024/11/03 04:32:54] ppocr INFO:         k : 50\n[2024/11/03 04:32:54] ppocr INFO:         name : DBHead\n[2024/11/03 04:32:54] ppocr INFO:     Neck : \n[2024/11/03 04:32:54] ppocr INFO:         name : RSEFPN\n[2024/11/03 04:32:54] ppocr INFO:         out_channels : 96\n[2024/11/03 04:32:54] ppocr INFO:         shortcut : True\n[2024/11/03 04:32:54] ppocr INFO:     Transform : None\n[2024/11/03 04:32:54] ppocr INFO:     algorithm : DB\n[2024/11/03 04:32:54] ppocr INFO:     model_type : det\n[2024/11/03 04:32:54] ppocr INFO: Eval : \n[2024/11/03 04:32:54] ppocr INFO:     dataset : \n[2024/11/03 04:32:54] ppocr INFO:         data_dir : /kaggle/input/scutctw1500-datasets/\n[2024/11/03 04:32:54] ppocr INFO:         label_file_list : ['/kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/test.txt']\n[2024/11/03 04:32:54] ppocr INFO:         name : SimpleDataSet\n[2024/11/03 04:32:54] ppocr INFO:         transforms : \n[2024/11/03 04:32:54] ppocr INFO:             DecodeImage : \n[2024/11/03 04:32:54] ppocr INFO:                 channel_first : False\n[2024/11/03 04:32:54] ppocr INFO:                 img_mode : BGR\n[2024/11/03 04:32:54] ppocr INFO:             DetLabelEncode : None\n[2024/11/03 04:32:54] ppocr INFO:             DetResizeForTest : None\n[2024/11/03 04:32:54] ppocr INFO:             NormalizeImage : \n[2024/11/03 04:32:54] ppocr INFO:                 mean : [0.485, 0.456, 0.406]\n[2024/11/03 04:32:54] ppocr INFO:                 order : hwc\n[2024/11/03 04:32:54] ppocr INFO:                 scale : 1./255.\n[2024/11/03 04:32:54] ppocr INFO:                 std : [0.229, 0.224, 0.225]\n[2024/11/03 04:32:54] ppocr INFO:             ToCHWImage : None\n[2024/11/03 04:32:54] ppocr INFO:             KeepKeys : \n[2024/11/03 04:32:54] ppocr INFO:                 keep_keys : ['image', 'shape', 'polys', 'ignore_tags']\n[2024/11/03 04:32:54] ppocr INFO:     loader : \n[2024/11/03 04:32:54] ppocr INFO:         batch_size_per_card : 1\n[2024/11/03 04:32:54] ppocr INFO:         drop_last : False\n[2024/11/03 04:32:54] ppocr INFO:         num_workers : 2\n[2024/11/03 04:32:54] ppocr INFO:         shuffle : False\n[2024/11/03 04:32:54] ppocr INFO: Global : \n[2024/11/03 04:32:54] ppocr INFO:     cal_metric_during_train : False\n[2024/11/03 04:32:54] ppocr INFO:     checkpoints : None\n[2024/11/03 04:32:54] ppocr INFO:     debug : False\n[2024/11/03 04:32:54] ppocr INFO:     distributed : True\n[2024/11/03 04:32:54] ppocr INFO:     epoch_num : 9\n[2024/11/03 04:32:54] ppocr INFO:     eval_batch_step : [0, 400]\n[2024/11/03 04:32:54] ppocr INFO:     infer_img : doc/imgs_en/img_10.jpg\n[2024/11/03 04:32:54] ppocr INFO:     log_smooth_window : 20\n[2024/11/03 04:32:54] ppocr INFO:     pretrained_model : /kaggle/input/ch-pp-ocrv3-det-distill-train/student.pdparams\n[2024/11/03 04:32:54] ppocr INFO:     print_batch_step : 10\n[2024/11/03 04:32:54] ppocr INFO:     save_epoch_step : 100\n[2024/11/03 04:32:54] ppocr INFO:     save_inference_dir : None\n[2024/11/03 04:32:54] ppocr INFO:     save_model_dir : ./output/ch_PP-OCR_V3_det/\n[2024/11/03 04:32:54] ppocr INFO:     save_res_path : ./checkpoints/det_db/predicts_db.txt\n[2024/11/03 04:32:54] ppocr INFO:     use_gpu : True\n[2024/11/03 04:32:54] ppocr INFO:     use_visualdl : False\n[2024/11/03 04:32:54] ppocr INFO: Loss : \n[2024/11/03 04:32:54] ppocr INFO:     alpha : 5\n[2024/11/03 04:32:54] ppocr INFO:     balance_loss : True\n[2024/11/03 04:32:54] ppocr INFO:     beta : 10\n[2024/11/03 04:32:54] ppocr INFO:     main_loss_type : DiceLoss\n[2024/11/03 04:32:54] ppocr INFO:     name : DBLoss\n[2024/11/03 04:32:54] ppocr INFO:     ohem_ratio : 3\n[2024/11/03 04:32:54] ppocr INFO: Metric : \n[2024/11/03 04:32:54] ppocr INFO:     main_indicator : hmean\n[2024/11/03 04:32:54] ppocr INFO:     name : DetMetric\n[2024/11/03 04:32:54] ppocr INFO: Optimizer : \n[2024/11/03 04:32:54] ppocr INFO:     beta1 : 0.9\n[2024/11/03 04:32:54] ppocr INFO:     beta2 : 0.999\n[2024/11/03 04:32:54] ppocr INFO:     lr : \n[2024/11/03 04:32:54] ppocr INFO:         learning_rate : 0.0001\n[2024/11/03 04:32:54] ppocr INFO:         name : Cosine\n[2024/11/03 04:32:54] ppocr INFO:         warmup_epoch : 2\n[2024/11/03 04:32:54] ppocr INFO:     name : Adam\n[2024/11/03 04:32:54] ppocr INFO:     regularizer : \n[2024/11/03 04:32:54] ppocr INFO:         factor : 5e-05\n[2024/11/03 04:32:54] ppocr INFO:         name : L2\n[2024/11/03 04:32:54] ppocr INFO: PostProcess : \n[2024/11/03 04:32:54] ppocr INFO:     box_thresh : 0.6\n[2024/11/03 04:32:54] ppocr INFO:     max_candidates : 1000\n[2024/11/03 04:32:54] ppocr INFO:     name : DBPostProcess\n[2024/11/03 04:32:54] ppocr INFO:     thresh : 0.3\n[2024/11/03 04:32:54] ppocr INFO:     unclip_ratio : 1.5\n[2024/11/03 04:32:54] ppocr INFO: Train : \n[2024/11/03 04:32:54] ppocr INFO:     dataset : \n[2024/11/03 04:32:54] ppocr INFO:         data_dir : /kaggle/input/scutctw1500-datasets/\n[2024/11/03 04:32:54] ppocr INFO:         label_file_list : ['/kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/training.txt']\n[2024/11/03 04:32:54] ppocr INFO:         name : SimpleDataSet\n[2024/11/03 04:32:54] ppocr INFO:         ratio_list : [1.0]\n[2024/11/03 04:32:54] ppocr INFO:         transforms : \n[2024/11/03 04:32:54] ppocr INFO:             DecodeImage : \n[2024/11/03 04:32:54] ppocr INFO:                 channel_first : False\n[2024/11/03 04:32:54] ppocr INFO:                 img_mode : BGR\n[2024/11/03 04:32:54] ppocr INFO:             DetLabelEncode : None\n[2024/11/03 04:32:54] ppocr INFO:             IaaAugment : \n[2024/11/03 04:32:54] ppocr INFO:                 augmenter_args : \n[2024/11/03 04:32:54] ppocr INFO:                     args : \n[2024/11/03 04:32:54] ppocr INFO:                         p : 0.5\n[2024/11/03 04:32:54] ppocr INFO:                     type : Fliplr\n[2024/11/03 04:32:54] ppocr INFO:                     args : \n[2024/11/03 04:32:54] ppocr INFO:                         rotate : [-10, 10]\n[2024/11/03 04:32:54] ppocr INFO:                     type : Affine\n[2024/11/03 04:32:54] ppocr INFO:                     args : \n[2024/11/03 04:32:54] ppocr INFO:                         size : [0.5, 3]\n[2024/11/03 04:32:54] ppocr INFO:                     type : Resize\n[2024/11/03 04:32:54] ppocr INFO:             EastRandomCropData : \n[2024/11/03 04:32:54] ppocr INFO:                 keep_ratio : True\n[2024/11/03 04:32:54] ppocr INFO:                 max_tries : 50\n[2024/11/03 04:32:54] ppocr INFO:                 size : [960, 960]\n[2024/11/03 04:32:54] ppocr INFO:             MakeBorderMap : \n[2024/11/03 04:32:54] ppocr INFO:                 shrink_ratio : 0.4\n[2024/11/03 04:32:54] ppocr INFO:                 thresh_max : 0.7\n[2024/11/03 04:32:54] ppocr INFO:                 thresh_min : 0.3\n[2024/11/03 04:32:54] ppocr INFO:             MakeShrinkMap : \n[2024/11/03 04:32:54] ppocr INFO:                 min_text_size : 8\n[2024/11/03 04:32:54] ppocr INFO:                 shrink_ratio : 0.4\n[2024/11/03 04:32:54] ppocr INFO:             NormalizeImage : \n[2024/11/03 04:32:54] ppocr INFO:                 mean : [0.485, 0.456, 0.406]\n[2024/11/03 04:32:54] ppocr INFO:                 order : hwc\n[2024/11/03 04:32:54] ppocr INFO:                 scale : 1./255.\n[2024/11/03 04:32:54] ppocr INFO:                 std : [0.229, 0.224, 0.225]\n[2024/11/03 04:32:54] ppocr INFO:             ToCHWImage : None\n[2024/11/03 04:32:54] ppocr INFO:             KeepKeys : \n[2024/11/03 04:32:54] ppocr INFO:                 keep_keys : ['image', 'threshold_map', 'threshold_mask', 'shrink_map', 'shrink_mask']\n[2024/11/03 04:32:54] ppocr INFO:     loader : \n[2024/11/03 04:32:54] ppocr INFO:         batch_size_per_card : 8\n[2024/11/03 04:32:54] ppocr INFO:         drop_last : False\n[2024/11/03 04:32:54] ppocr INFO:         num_workers : 4\n[2024/11/03 04:32:54] ppocr INFO:         shuffle : True\n[2024/11/03 04:32:54] ppocr INFO: profiler_options : None\n[2024/11/03 04:32:54] ppocr INFO: train with paddle 2.5.0 and device Place(gpu:0)\n======================= Modified FLAGS detected =======================\nFLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')\n=======================================================================\nI1103 04:32:54.189285   142 tcp_utils.cc:181] The server starts to listen on IP_ANY:35737\nI1103 04:32:54.189476   142 tcp_utils.cc:130] Successfully connected to 172.19.2.2:35737\nW1103 04:32:57.412665   142 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.4, Runtime API Version: 11.8\nW1103 04:32:57.413568   142 gpu_resources.cc:149] device: 0, cuDNN Version: 90.0.\n[2024/11/03 04:32:57] ppocr INFO: Initialize indexs of datasets:['/kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/training.txt']\n[2024/11/03 04:32:57] ppocr INFO: Initialize indexs of datasets:['/kaggle/input/scut-ctw1500-annotations-supported-by-paddleocr-2/test.txt']\n[2024/11/03 04:32:58] ppocr INFO: train dataloader has 63 iters\n[2024/11/03 04:32:58] ppocr INFO: valid dataloader has 500 iters\n[2024/11/03 04:32:58] ppocr INFO: load pretrain successful from /kaggle/input/ch-pp-ocrv3-det-distill-train/student\n[2024/11/03 04:32:58] ppocr INFO: During the training process, after the 0th iteration, an evaluation is run every 400 iterations\n[2024/11/03 04:33:37] ppocr INFO: epoch: [1/9], global_step: 10, lr: 0.000004, loss: 2.625730, loss_shrink_maps: 1.582476, loss_threshold_maps: 0.737675, loss_binary_maps: 0.316313, loss_cbn: 0.000000, avg_reader_cost: 3.01776 s, avg_batch_cost: 3.86246 s, avg_samples: 8.0, ips: 2.07122 samples/s, eta: 0:35:51, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:33:57] ppocr INFO: epoch: [1/9], global_step: 20, lr: 0.000008, loss: 2.592360, loss_shrink_maps: 1.592752, loss_threshold_maps: 0.688670, loss_binary_maps: 0.318778, loss_cbn: 0.000000, avg_reader_cost: 1.25730 s, avg_batch_cost: 1.77629 s, avg_samples: 8.0, ips: 4.50377 samples/s, eta: 0:25:42, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:34:32] ppocr INFO: epoch: [1/9], global_step: 30, lr: 0.000015, loss: 2.311398, loss_shrink_maps: 1.393297, loss_threshold_maps: 0.656707, loss_binary_maps: 0.278827, loss_cbn: 0.000000, avg_reader_cost: 2.72139 s, avg_batch_cost: 3.33408 s, avg_samples: 8.0, ips: 2.39946 samples/s, eta: 0:26:46, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:34:52] ppocr INFO: epoch: [1/9], global_step: 40, lr: 0.000023, loss: 2.240226, loss_shrink_maps: 1.345361, loss_threshold_maps: 0.664764, loss_binary_maps: 0.268746, loss_cbn: 0.000000, avg_reader_cost: 1.23896 s, avg_batch_cost: 1.81276 s, avg_samples: 8.0, ips: 4.41317 samples/s, eta: 0:23:41, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:35:22] ppocr INFO: epoch: [1/9], global_step: 50, lr: 0.000031, loss: 2.233117, loss_shrink_maps: 1.382743, loss_threshold_maps: 0.626239, loss_binary_maps: 0.276521, loss_cbn: 0.000000, avg_reader_cost: 2.10767 s, avg_batch_cost: 2.69990 s, avg_samples: 8.0, ips: 2.96307 samples/s, eta: 0:23:14, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:35:41] ppocr INFO: epoch: [1/9], global_step: 60, lr: 0.000039, loss: 2.317220, loss_shrink_maps: 1.402420, loss_threshold_maps: 0.621249, loss_binary_maps: 0.280584, loss_cbn: 0.000000, avg_reader_cost: 1.07282 s, avg_batch_cost: 1.57645 s, avg_samples: 8.0, ips: 5.07469 samples/s, eta: 0:21:12, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:35:47] ppocr INFO: epoch: [1/9], global_step: 63, lr: 0.000042, loss: 2.317220, loss_shrink_maps: 1.402420, loss_threshold_maps: 0.635238, loss_binary_maps: 0.280584, loss_cbn: 0.000000, avg_reader_cost: 0.39595 s, avg_batch_cost: 0.47731 s, avg_samples: 2.0, ips: 4.19012 samples/s, eta: 0:20:43, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:35:47] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:36:15] ppocr INFO: epoch: [2/9], global_step: 70, lr: 0.000047, loss: 2.395437, loss_shrink_maps: 1.430268, loss_threshold_maps: 0.665345, loss_binary_maps: 0.285744, loss_cbn: 0.000000, avg_reader_cost: 2.02815 s, avg_batch_cost: 2.41918 s, avg_samples: 5.6, ips: 2.31484 samples/s, eta: 0:21:15, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:36:41] ppocr INFO: epoch: [2/9], global_step: 80, lr: 0.000055, loss: 2.363410, loss_shrink_maps: 1.422780, loss_threshold_maps: 0.672981, loss_binary_maps: 0.284517, loss_cbn: 0.000000, avg_reader_cost: 1.92228 s, avg_batch_cost: 2.56532 s, avg_samples: 8.0, ips: 3.11852 samples/s, eta: 0:20:49, max_mem_reserved: 6488 MB, max_mem_allocated: 5426 MB\n[2024/11/03 04:37:10] ppocr INFO: epoch: [2/9], global_step: 90, lr: 0.000063, loss: 2.333830, loss_shrink_maps: 1.406721, loss_threshold_maps: 0.669343, loss_binary_maps: 0.281163, loss_cbn: 0.000000, avg_reader_cost: 1.75030 s, avg_batch_cost: 2.35424 s, avg_samples: 8.0, ips: 3.39813 samples/s, eta: 0:20:12, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:37:37] ppocr INFO: epoch: [2/9], global_step: 100, lr: 0.000071, loss: 2.499909, loss_shrink_maps: 1.495040, loss_threshold_maps: 0.695264, loss_binary_maps: 0.298621, loss_cbn: 0.000000, avg_reader_cost: 1.64612 s, avg_batch_cost: 2.22083 s, avg_samples: 8.0, ips: 3.60226 samples/s, eta: 0:19:32, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:38:05] ppocr INFO: epoch: [2/9], global_step: 110, lr: 0.000079, loss: 2.350222, loss_shrink_maps: 1.369941, loss_threshold_maps: 0.692006, loss_binary_maps: 0.273916, loss_cbn: 0.000000, avg_reader_cost: 1.87359 s, avg_batch_cost: 2.51999 s, avg_samples: 8.0, ips: 3.17461 samples/s, eta: 0:19:07, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:38:30] ppocr INFO: epoch: [2/9], global_step: 120, lr: 0.000087, loss: 2.147838, loss_shrink_maps: 1.272873, loss_threshold_maps: 0.646919, loss_binary_maps: 0.254500, loss_cbn: 0.000000, avg_reader_cost: 1.73654 s, avg_batch_cost: 2.33431 s, avg_samples: 8.0, ips: 3.42714 samples/s, eta: 0:18:35, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:38:36] ppocr INFO: epoch: [2/9], global_step: 126, lr: 0.000092, loss: 2.132823, loss_shrink_maps: 1.272951, loss_threshold_maps: 0.623696, loss_binary_maps: 0.254266, loss_cbn: 0.000000, avg_reader_cost: 0.29698 s, avg_batch_cost: 0.50833 s, avg_samples: 4.4, ips: 8.65579 samples/s, eta: 0:17:46, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:38:37] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:38:56] ppocr INFO: epoch: [3/9], global_step: 130, lr: 0.000095, loss: 2.262186, loss_shrink_maps: 1.344731, loss_threshold_maps: 0.631894, loss_binary_maps: 0.268364, loss_cbn: 0.000000, avg_reader_cost: 1.64887 s, avg_batch_cost: 1.87177 s, avg_samples: 3.2, ips: 1.70962 samples/s, eta: 0:18:06, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:39:28] ppocr INFO: epoch: [3/9], global_step: 140, lr: 0.000100, loss: 2.236050, loss_shrink_maps: 1.358324, loss_threshold_maps: 0.616536, loss_binary_maps: 0.269790, loss_cbn: 0.000000, avg_reader_cost: 1.93969 s, avg_batch_cost: 2.50238 s, avg_samples: 8.0, ips: 3.19695 samples/s, eta: 0:17:42, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:39:50] ppocr INFO: epoch: [3/9], global_step: 150, lr: 0.000100, loss: 2.135771, loss_shrink_maps: 1.262699, loss_threshold_maps: 0.616536, loss_binary_maps: 0.253087, loss_cbn: 0.000000, avg_reader_cost: 1.23326 s, avg_batch_cost: 1.78648 s, avg_samples: 8.0, ips: 4.47808 samples/s, eta: 0:16:58, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:40:18] ppocr INFO: epoch: [3/9], global_step: 160, lr: 0.000100, loss: 2.129953, loss_shrink_maps: 1.259897, loss_threshold_maps: 0.624529, loss_binary_maps: 0.252346, loss_cbn: 0.000000, avg_reader_cost: 1.56159 s, avg_batch_cost: 2.11042 s, avg_samples: 8.0, ips: 3.79072 samples/s, eta: 0:16:25, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:40:47] ppocr INFO: epoch: [3/9], global_step: 170, lr: 0.000099, loss: 2.151436, loss_shrink_maps: 1.299210, loss_threshold_maps: 0.636451, loss_binary_maps: 0.259624, loss_cbn: 0.000000, avg_reader_cost: 1.93246 s, avg_batch_cost: 2.51588 s, avg_samples: 8.0, ips: 3.17980 samples/s, eta: 0:16:03, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:41:14] ppocr INFO: epoch: [3/9], global_step: 180, lr: 0.000099, loss: 2.251417, loss_shrink_maps: 1.313658, loss_threshold_maps: 0.629553, loss_binary_maps: 0.262893, loss_cbn: 0.000000, avg_reader_cost: 1.86232 s, avg_batch_cost: 2.41027 s, avg_samples: 8.0, ips: 3.31913 samples/s, eta: 0:15:38, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:41:25] ppocr INFO: epoch: [3/9], global_step: 189, lr: 0.000098, loss: 2.351348, loss_shrink_maps: 1.410176, loss_threshold_maps: 0.643906, loss_binary_maps: 0.282094, loss_cbn: 0.000000, avg_reader_cost: 0.52533 s, avg_batch_cost: 0.87673 s, avg_samples: 6.8, ips: 7.75607 samples/s, eta: 0:14:50, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:41:25] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:41:36] ppocr INFO: epoch: [4/9], global_step: 190, lr: 0.000098, loss: 2.351348, loss_shrink_maps: 1.410176, loss_threshold_maps: 0.643906, loss_binary_maps: 0.282094, loss_cbn: 0.000000, avg_reader_cost: 1.04584 s, avg_batch_cost: 1.10682 s, avg_samples: 0.8, ips: 0.72279 samples/s, eta: 0:15:05, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:42:08] ppocr INFO: epoch: [4/9], global_step: 200, lr: 0.000097, loss: 2.151351, loss_shrink_maps: 1.252916, loss_threshold_maps: 0.639604, loss_binary_maps: 0.250106, loss_cbn: 0.000000, avg_reader_cost: 2.08022 s, avg_batch_cost: 2.70693 s, avg_samples: 8.0, ips: 2.95537 samples/s, eta: 0:14:47, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:42:40] ppocr INFO: epoch: [4/9], global_step: 210, lr: 0.000096, loss: 2.063997, loss_shrink_maps: 1.206146, loss_threshold_maps: 0.642379, loss_binary_maps: 0.241443, loss_cbn: 0.000000, avg_reader_cost: 2.43621 s, avg_batch_cost: 3.04598 s, avg_samples: 8.0, ips: 2.62642 samples/s, eta: 0:14:33, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:43:03] ppocr INFO: epoch: [4/9], global_step: 220, lr: 0.000095, loss: 2.116390, loss_shrink_maps: 1.252069, loss_threshold_maps: 0.617974, loss_binary_maps: 0.250008, loss_cbn: 0.000000, avg_reader_cost: 1.50384 s, avg_batch_cost: 2.08023 s, avg_samples: 8.0, ips: 3.84572 samples/s, eta: 0:14:03, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:43:34] ppocr INFO: epoch: [4/9], global_step: 230, lr: 0.000093, loss: 2.179658, loss_shrink_maps: 1.315474, loss_threshold_maps: 0.614312, loss_binary_maps: 0.263777, loss_cbn: 0.000000, avg_reader_cost: 2.35932 s, avg_batch_cost: 3.04992 s, avg_samples: 8.0, ips: 2.62302 samples/s, eta: 0:13:48, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:43:57] ppocr INFO: epoch: [4/9], global_step: 240, lr: 0.000092, loss: 2.210261, loss_shrink_maps: 1.308718, loss_threshold_maps: 0.652272, loss_binary_maps: 0.262488, loss_cbn: 0.000000, avg_reader_cost: 1.25722 s, avg_batch_cost: 1.76038 s, avg_samples: 8.0, ips: 4.54447 samples/s, eta: 0:13:14, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:44:21] ppocr INFO: epoch: [4/9], global_step: 250, lr: 0.000090, loss: 2.152354, loss_shrink_maps: 1.274938, loss_threshold_maps: 0.643933, loss_binary_maps: 0.254886, loss_cbn: 0.000000, avg_reader_cost: 1.67055 s, avg_batch_cost: 2.11379 s, avg_samples: 8.0, ips: 3.78467 samples/s, eta: 0:12:45, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:44:22] ppocr INFO: epoch: [4/9], global_step: 252, lr: 0.000090, loss: 2.119335, loss_shrink_maps: 1.238672, loss_threshold_maps: 0.621599, loss_binary_maps: 0.247963, loss_cbn: 0.000000, avg_reader_cost: 0.00037 s, avg_batch_cost: 0.04410 s, avg_samples: 1.2, ips: 27.21087 samples/s, eta: 0:12:35, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:44:22] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:44:50] ppocr INFO: epoch: [5/9], global_step: 260, lr: 0.000089, loss: 2.052530, loss_shrink_maps: 1.198041, loss_threshold_maps: 0.608119, loss_binary_maps: 0.239578, loss_cbn: 0.000000, avg_reader_cost: 2.27624 s, avg_batch_cost: 2.75264 s, avg_samples: 6.4, ips: 2.32504 samples/s, eta: 0:12:26, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:45:25] ppocr INFO: epoch: [5/9], global_step: 270, lr: 0.000087, loss: 2.038088, loss_shrink_maps: 1.181046, loss_threshold_maps: 0.617039, loss_binary_maps: 0.236133, loss_cbn: 0.000000, avg_reader_cost: 2.76890 s, avg_batch_cost: 3.32540 s, avg_samples: 8.0, ips: 2.40573 samples/s, eta: 0:12:11, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:45:44] ppocr INFO: epoch: [5/9], global_step: 280, lr: 0.000085, loss: 1.996851, loss_shrink_maps: 1.160551, loss_threshold_maps: 0.617039, loss_binary_maps: 0.232349, loss_cbn: 0.000000, avg_reader_cost: 1.22277 s, avg_batch_cost: 1.75641 s, avg_samples: 8.0, ips: 4.55474 samples/s, eta: 0:11:39, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:46:18] ppocr INFO: epoch: [5/9], global_step: 290, lr: 0.000083, loss: 2.169122, loss_shrink_maps: 1.305665, loss_threshold_maps: 0.597343, loss_binary_maps: 0.261800, loss_cbn: 0.000000, avg_reader_cost: 2.69256 s, avg_batch_cost: 3.21039 s, avg_samples: 8.0, ips: 2.49191 samples/s, eta: 0:11:22, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:46:43] ppocr INFO: epoch: [5/9], global_step: 300, lr: 0.000081, loss: 2.186798, loss_shrink_maps: 1.333531, loss_threshold_maps: 0.616924, loss_binary_maps: 0.267025, loss_cbn: 0.000000, avg_reader_cost: 1.85622 s, avg_batch_cost: 2.33086 s, avg_samples: 8.0, ips: 3.43222 samples/s, eta: 0:10:56, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:47:07] ppocr INFO: epoch: [5/9], global_step: 310, lr: 0.000079, loss: 2.235190, loss_shrink_maps: 1.343430, loss_threshold_maps: 0.632028, loss_binary_maps: 0.268855, loss_cbn: 0.000000, avg_reader_cost: 1.57206 s, avg_batch_cost: 2.20297 s, avg_samples: 8.0, ips: 3.63146 samples/s, eta: 0:10:30, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:47:13] ppocr INFO: epoch: [5/9], global_step: 315, lr: 0.000077, loss: 2.226589, loss_shrink_maps: 1.315827, loss_threshold_maps: 0.615102, loss_binary_maps: 0.263401, loss_cbn: 0.000000, avg_reader_cost: 0.29570 s, avg_batch_cost: 0.45185 s, avg_samples: 3.6, ips: 7.96726 samples/s, eta: 0:10:11, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:47:13] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:47:35] ppocr INFO: epoch: [6/9], global_step: 320, lr: 0.000076, loss: 2.190357, loss_shrink_maps: 1.310976, loss_threshold_maps: 0.609789, loss_binary_maps: 0.261971, loss_cbn: 0.000000, avg_reader_cost: 1.65239 s, avg_batch_cost: 1.97758 s, avg_samples: 4.0, ips: 2.02268 samples/s, eta: 0:10:05, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:48:02] ppocr INFO: epoch: [6/9], global_step: 330, lr: 0.000074, loss: 2.058090, loss_shrink_maps: 1.232558, loss_threshold_maps: 0.611479, loss_binary_maps: 0.246479, loss_cbn: 0.000000, avg_reader_cost: 1.47726 s, avg_batch_cost: 2.10563 s, avg_samples: 8.0, ips: 3.79934 samples/s, eta: 0:09:38, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:48:23] ppocr INFO: epoch: [6/9], global_step: 340, lr: 0.000071, loss: 2.081484, loss_shrink_maps: 1.232558, loss_threshold_maps: 0.624323, loss_binary_maps: 0.246479, loss_cbn: 0.000000, avg_reader_cost: 0.96634 s, avg_batch_cost: 1.49942 s, avg_samples: 8.0, ips: 5.33540 samples/s, eta: 0:09:07, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:48:54] ppocr INFO: epoch: [6/9], global_step: 350, lr: 0.000069, loss: 2.144902, loss_shrink_maps: 1.238794, loss_threshold_maps: 0.625834, loss_binary_maps: 0.247815, loss_cbn: 0.000000, avg_reader_cost: 1.24869 s, avg_batch_cost: 1.82853 s, avg_samples: 8.0, ips: 4.37510 samples/s, eta: 0:08:40, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:49:20] ppocr INFO: epoch: [6/9], global_step: 360, lr: 0.000066, loss: 2.225792, loss_shrink_maps: 1.289856, loss_threshold_maps: 0.648004, loss_binary_maps: 0.258069, loss_cbn: 0.000000, avg_reader_cost: 1.34400 s, avg_batch_cost: 1.87100 s, avg_samples: 8.0, ips: 4.27578 samples/s, eta: 0:08:13, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:49:49] ppocr INFO: epoch: [6/9], global_step: 370, lr: 0.000064, loss: 2.212562, loss_shrink_maps: 1.329945, loss_threshold_maps: 0.630948, loss_binary_maps: 0.267370, loss_cbn: 0.000000, avg_reader_cost: 0.36812 s, avg_batch_cost: 0.91501 s, avg_samples: 8.0, ips: 8.74309 samples/s, eta: 0:07:41, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:50:00] ppocr INFO: epoch: [6/9], global_step: 378, lr: 0.000062, loss: 2.167378, loss_shrink_maps: 1.304021, loss_threshold_maps: 0.600547, loss_binary_maps: 0.261007, loss_cbn: 0.000000, avg_reader_cost: 0.06977 s, avg_batch_cost: 0.35862 s, avg_samples: 6.0, ips: 16.73094 samples/s, eta: 0:07:15, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:50:00] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:50:18] ppocr INFO: epoch: [7/9], global_step: 380, lr: 0.000061, loss: 2.212562, loss_shrink_maps: 1.329945, loss_threshold_maps: 0.619732, loss_binary_maps: 0.267370, loss_cbn: 0.000000, avg_reader_cost: 1.63883 s, avg_batch_cost: 1.79571 s, avg_samples: 1.6, ips: 0.89101 samples/s, eta: 0:07:17, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:50:43] ppocr INFO: epoch: [7/9], global_step: 390, lr: 0.000058, loss: 2.163812, loss_shrink_maps: 1.273323, loss_threshold_maps: 0.644569, loss_binary_maps: 0.254687, loss_cbn: 0.000000, avg_reader_cost: 1.74186 s, avg_batch_cost: 2.30005 s, avg_samples: 8.0, ips: 3.47819 samples/s, eta: 0:06:53, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:51:13] ppocr INFO: epoch: [7/9], global_step: 400, lr: 0.000056, loss: 2.021738, loss_shrink_maps: 1.194202, loss_threshold_maps: 0.611192, loss_binary_maps: 0.238939, loss_cbn: 0.000000, avg_reader_cost: 2.15325 s, avg_batch_cost: 2.75642 s, avg_samples: 8.0, ips: 2.90232 samples/s, eta: 0:06:31, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n\neval model::   0%|          | 0/500 [00:00&lt;?, ?it/s]\neval model::   0%|          | 1/500 [00:00&lt;06:55,  1.20it/s]\neval model::   0%|          | 2/500 [00:01&lt;03:55,  2.12it/s]\neval model::   1%|          | 3/500 [00:01&lt;03:27,  2.39it/s]\neval model::   1%|          | 4/500 [00:01&lt;02:45,  3.00it/s]\neval model::   1%|          | 5/500 [00:01&lt;02:41,  3.07it/s]\neval model::   1%|          | 6/500 [00:02&lt;02:17,  3.60it/s]\neval model::   1%|▏         | 7/500 [00:02&lt;01:54,  4.31it/s]\neval model::   2%|▏         | 8/500 [00:02&lt;01:40,  4.92it/s]\neval model::   2%|▏         | 9/500 [00:02&lt;02:34,  3.19it/s]\neval model::   2%|▏         | 11/500 [00:03&lt;01:41,  4.81it/s]\neval model::   2%|▏         | 12/500 [00:03&lt;02:13,  3.66it/s]\neval model::   3%|▎         | 13/500 [00:03&lt;02:09,  3.75it/s]\neval model::   3%|▎         | 14/500 [00:04&lt;02:04,  3.90it/s]\neval model::   3%|▎         | 15/500 [00:04&lt;02:18,  3.50it/s]\neval model::   3%|▎         | 16/500 [00:05&lt;03:36,  2.23it/s]\neval model::   3%|▎         | 17/500 [00:05&lt;02:56,  2.73it/s]\neval model::   4%|▎         | 18/500 [00:05&lt;02:27,  3.26it/s]\neval model::   4%|▍         | 19/500 [00:05&lt;02:03,  3.89it/s]\neval model::   4%|▍         | 20/500 [00:05&lt;01:59,  4.02it/s]\neval model::   4%|▍         | 21/500 [00:06&lt;01:51,  4.29it/s]\neval model::   4%|▍         | 22/500 [00:06&lt;01:51,  4.29it/s]\neval model::   5%|▍         | 23/500 [00:06&lt;01:41,  4.69it/s]\neval model::   5%|▍         | 24/500 [00:06&lt;01:36,  4.94it/s]\neval model::   5%|▌         | 25/500 [00:06&lt;01:30,  5.26it/s]\neval model::   5%|▌         | 26/500 [00:07&lt;01:21,  5.78it/s]\neval model::   5%|▌         | 27/500 [00:07&lt;01:22,  5.70it/s]\neval model::   6%|▌         | 28/500 [00:07&lt;01:24,  5.57it/s]\neval model::   6%|▌         | 29/500 [00:07&lt;01:16,  6.20it/s]\neval model::   6%|▌         | 30/500 [00:08&lt;02:06,  3.72it/s]\neval model::   6%|▌         | 31/500 [00:08&lt;01:48,  4.31it/s]\neval model::   6%|▋         | 32/500 [00:08&lt;01:40,  4.65it/s]\neval model::   7%|▋         | 34/500 [00:08&lt;01:19,  5.86it/s]\neval model::   7%|▋         | 36/500 [00:08&lt;01:02,  7.46it/s]\neval model::   7%|▋         | 37/500 [00:08&lt;00:59,  7.75it/s]\neval model::   8%|▊         | 39/500 [00:09&lt;00:49,  9.25it/s]\neval model::   8%|▊         | 40/500 [00:09&lt;01:01,  7.46it/s]\neval model::   8%|▊         | 42/500 [00:09&lt;00:54,  8.43it/s]\neval model::   9%|▊         | 43/500 [00:09&lt;00:53,  8.60it/s]\neval model::   9%|▉         | 44/500 [00:10&lt;01:47,  4.24it/s]\neval model::   9%|▉         | 45/500 [00:10&lt;01:35,  4.76it/s]\neval model::   9%|▉         | 46/500 [00:10&lt;01:26,  5.28it/s]\neval model::  10%|▉         | 48/500 [00:10&lt;01:03,  7.10it/s]\neval model::  10%|█         | 51/500 [00:10&lt;00:46,  9.60it/s]\neval model::  11%|█         | 53/500 [00:10&lt;00:41, 10.89it/s]\neval model::  11%|█         | 55/500 [00:11&lt;00:37, 11.89it/s]\neval model::  11%|█▏        | 57/500 [00:11&lt;00:35, 12.53it/s]\neval model::  12%|█▏        | 59/500 [00:11&lt;00:44,  9.90it/s]\neval model::  12%|█▏        | 61/500 [00:11&lt;00:42, 10.38it/s]\neval model::  13%|█▎        | 63/500 [00:11&lt;00:43, 10.10it/s]\neval model::  13%|█▎        | 65/500 [00:12&lt;00:39, 11.03it/s]\neval model::  13%|█▎        | 67/500 [00:12&lt;00:34, 12.51it/s]\neval model::  14%|█▍        | 69/500 [00:12&lt;00:30, 13.97it/s]\neval model::  14%|█▍        | 71/500 [00:12&lt;00:30, 14.17it/s]\neval model::  15%|█▍        | 73/500 [00:12&lt;00:29, 14.50it/s]\neval model::  15%|█▌        | 75/500 [00:12&lt;00:27, 15.40it/s]\neval model::  15%|█▌        | 77/500 [00:12&lt;00:33, 12.46it/s]\neval model::  16%|█▌        | 80/500 [00:12&lt;00:26, 16.07it/s]\neval model::  16%|█▋        | 82/500 [00:13&lt;00:27, 15.17it/s]\neval model::  17%|█▋        | 84/500 [00:13&lt;00:25, 16.07it/s]\neval model::  17%|█▋        | 87/500 [00:13&lt;00:22, 18.62it/s]\neval model::  18%|█▊        | 89/500 [00:13&lt;00:21, 18.85it/s]\neval model::  18%|█▊        | 91/500 [00:13&lt;00:22, 18.16it/s]\neval model::  19%|█▊        | 93/500 [00:13&lt;00:22, 17.76it/s]\neval model::  19%|█▉        | 96/500 [00:13&lt;00:20, 19.75it/s]\neval model::  20%|█▉        | 99/500 [00:13&lt;00:21, 18.94it/s]\neval model::  20%|██        | 101/500 [00:14&lt;00:21, 18.73it/s]\neval model::  21%|██        | 103/500 [00:14&lt;00:23, 17.02it/s]\neval model::  21%|██        | 106/500 [00:14&lt;00:20, 18.77it/s]\neval model::  22%|██▏       | 109/500 [00:14&lt;00:20, 18.92it/s]\neval model::  22%|██▏       | 112/500 [00:14&lt;00:19, 19.91it/s]\neval model::  23%|██▎       | 115/500 [00:14&lt;00:18, 20.43it/s]\neval model::  24%|██▎       | 118/500 [00:14&lt;00:17, 22.08it/s]\neval model::  24%|██▍       | 121/500 [00:15&lt;00:17, 22.12it/s]\neval model::  25%|██▍       | 124/500 [00:15&lt;00:19, 19.67it/s]\neval model::  25%|██▌       | 127/500 [00:15&lt;00:18, 20.36it/s]\neval model::  26%|██▌       | 130/500 [00:15&lt;00:18, 20.47it/s]\neval model::  27%|██▋       | 133/500 [00:15&lt;00:18, 20.38it/s]\neval model::  27%|██▋       | 136/500 [00:15&lt;00:17, 21.04it/s]\neval model::  28%|██▊       | 139/500 [00:15&lt;00:18, 19.49it/s]\neval model::  28%|██▊       | 142/500 [00:16&lt;00:17, 20.35it/s]\neval model::  29%|██▉       | 145/500 [00:16&lt;00:17, 20.41it/s]\neval model::  30%|██▉       | 148/500 [00:16&lt;00:18, 19.52it/s]\neval model::  30%|███       | 150/500 [00:16&lt;00:17, 19.59it/s]\neval model::  30%|███       | 152/500 [00:16&lt;00:19, 18.31it/s]\neval model::  31%|███       | 155/500 [00:16&lt;00:17, 19.41it/s]\neval model::  31%|███▏      | 157/500 [00:16&lt;00:22, 14.97it/s]\neval model::  32%|███▏      | 160/500 [00:17&lt;00:19, 17.33it/s]\neval model::  33%|███▎      | 163/500 [00:17&lt;00:17, 19.43it/s]\neval model::  33%|███▎      | 166/500 [00:17&lt;00:15, 20.89it/s]\neval model::  34%|███▍      | 169/500 [00:17&lt;00:15, 21.05it/s]\neval model::  34%|███▍      | 172/500 [00:17&lt;00:14, 22.90it/s]\neval model::  35%|███▌      | 175/500 [00:17&lt;00:14, 22.84it/s]\neval model::  36%|███▌      | 178/500 [00:17&lt;00:13, 23.55it/s]\neval model::  36%|███▌      | 181/500 [00:17&lt;00:12, 25.14it/s]\neval model::  37%|███▋      | 184/500 [00:18&lt;00:13, 23.82it/s]\neval model::  37%|███▋      | 187/500 [00:18&lt;00:12, 24.16it/s]\neval model::  38%|███▊      | 190/500 [00:18&lt;00:13, 22.38it/s]\neval model::  39%|███▊      | 193/500 [00:18&lt;00:14, 21.89it/s]\neval model::  39%|███▉      | 196/500 [00:18&lt;00:13, 22.71it/s]\neval model::  40%|███▉      | 199/500 [00:18&lt;00:13, 22.41it/s]\neval model::  40%|████      | 202/500 [00:18&lt;00:12, 23.69it/s]\neval model::  41%|████      | 205/500 [00:19&lt;00:15, 19.36it/s]\neval model::  42%|████▏     | 208/500 [00:19&lt;00:14, 20.61it/s]\neval model::  42%|████▏     | 211/500 [00:19&lt;00:13, 20.78it/s]\neval model::  43%|████▎     | 214/500 [00:19&lt;00:13, 21.78it/s]\neval model::  43%|████▎     | 217/500 [00:19&lt;00:12, 23.44it/s]\neval model::  44%|████▍     | 220/500 [00:19&lt;00:12, 23.07it/s]\neval model::  45%|████▍     | 223/500 [00:19&lt;00:11, 23.56it/s]\neval model::  45%|████▌     | 226/500 [00:19&lt;00:11, 23.83it/s]\neval model::  46%|████▌     | 229/500 [00:20&lt;00:13, 20.54it/s]\neval model::  46%|████▋     | 232/500 [00:20&lt;00:13, 20.36it/s]\neval model::  47%|████▋     | 235/500 [00:20&lt;00:12, 21.41it/s]\neval model::  48%|████▊     | 238/500 [00:20&lt;00:12, 21.43it/s]\neval model::  48%|████▊     | 241/500 [00:20&lt;00:11, 22.50it/s]\neval model::  49%|████▉     | 244/500 [00:20&lt;00:11, 22.90it/s]\neval model::  49%|████▉     | 247/500 [00:20&lt;00:10, 23.66it/s]\neval model::  50%|█████     | 250/500 [00:21&lt;00:10, 24.48it/s]\neval model::  51%|█████     | 253/500 [00:21&lt;00:09, 25.38it/s]\neval model::  51%|█████     | 256/500 [00:21&lt;00:10, 23.24it/s]\neval model::  52%|█████▏    | 259/500 [00:21&lt;00:10, 23.25it/s]\neval model::  52%|█████▏    | 262/500 [00:21&lt;00:10, 23.78it/s]\neval model::  53%|█████▎    | 265/500 [00:21&lt;00:09, 24.68it/s]\neval model::  54%|█████▎    | 268/500 [00:21&lt;00:09, 25.56it/s]\neval model::  54%|█████▍    | 271/500 [00:21&lt;00:09, 24.45it/s]\neval model::  55%|█████▍    | 274/500 [00:22&lt;00:09, 24.45it/s]\neval model::  55%|█████▌    | 277/500 [00:22&lt;00:09, 24.06it/s]\neval model::  56%|█████▌    | 280/500 [00:22&lt;00:09, 24.31it/s]\neval model::  57%|█████▋    | 283/500 [00:22&lt;00:09, 23.55it/s]\neval model::  57%|█████▋    | 286/500 [00:22&lt;00:08, 24.47it/s]\neval model::  58%|█████▊    | 289/500 [00:22&lt;00:09, 22.93it/s]\neval model::  58%|█████▊    | 292/500 [00:22&lt;00:09, 23.06it/s]\neval model::  59%|█████▉    | 295/500 [00:22&lt;00:09, 21.39it/s]\neval model::  60%|█████▉    | 298/500 [00:23&lt;00:09, 21.01it/s]\neval model::  60%|██████    | 301/500 [00:23&lt;00:10, 18.47it/s]\neval model::  61%|██████    | 304/500 [00:23&lt;00:09, 20.36it/s]\neval model::  61%|██████▏   | 307/500 [00:23&lt;00:09, 19.81it/s]\neval model::  62%|██████▏   | 310/500 [00:23&lt;00:09, 20.57it/s]\neval model::  63%|██████▎   | 313/500 [00:23&lt;00:09, 20.25it/s]\neval model::  63%|██████▎   | 316/500 [00:24&lt;00:09, 20.09it/s]\neval model::  64%|██████▍   | 319/500 [00:24&lt;00:08, 21.80it/s]\neval model::  64%|██████▍   | 322/500 [00:24&lt;00:08, 19.90it/s]\neval model::  65%|██████▌   | 325/500 [00:24&lt;00:08, 20.17it/s]\neval model::  66%|██████▌   | 328/500 [00:24&lt;00:08, 20.66it/s]\neval model::  66%|██████▌   | 331/500 [00:24&lt;00:07, 21.45it/s]\neval model::  67%|██████▋   | 334/500 [00:24&lt;00:07, 22.05it/s]\neval model::  67%|██████▋   | 337/500 [00:25&lt;00:08, 19.80it/s]\neval model::  68%|██████▊   | 340/500 [00:25&lt;00:07, 20.31it/s]\neval model::  69%|██████▊   | 343/500 [00:25&lt;00:07, 22.02it/s]\neval model::  69%|██████▉   | 346/500 [00:25&lt;00:07, 20.40it/s]\neval model::  70%|██████▉   | 349/500 [00:25&lt;00:07, 20.99it/s]\neval model::  70%|███████   | 352/500 [00:25&lt;00:07, 20.44it/s]\neval model::  71%|███████   | 355/500 [00:25&lt;00:06, 21.23it/s]\neval model::  72%|███████▏  | 358/500 [00:26&lt;00:06, 21.95it/s]\neval model::  72%|███████▏  | 361/500 [00:26&lt;00:06, 23.11it/s]\neval model::  73%|███████▎  | 364/500 [00:26&lt;00:06, 21.97it/s]\neval model::  73%|███████▎  | 367/500 [00:26&lt;00:05, 22.67it/s]\neval model::  74%|███████▍  | 370/500 [00:26&lt;00:06, 20.52it/s]\neval model::  75%|███████▍  | 373/500 [00:26&lt;00:05, 22.24it/s]\neval model::  75%|███████▌  | 376/500 [00:26&lt;00:05, 21.59it/s]\neval model::  76%|███████▌  | 379/500 [00:26&lt;00:05, 22.09it/s]\neval model::  76%|███████▋  | 382/500 [00:27&lt;00:05, 21.17it/s]\neval model::  77%|███████▋  | 385/500 [00:27&lt;00:05, 21.56it/s]\neval model::  78%|███████▊  | 388/500 [00:27&lt;00:05, 20.60it/s]\neval model::  78%|███████▊  | 391/500 [00:27&lt;00:04, 22.19it/s]\neval model::  79%|███████▉  | 394/500 [00:27&lt;00:04, 22.19it/s]\neval model::  79%|███████▉  | 397/500 [00:27&lt;00:04, 23.53it/s]\neval model::  80%|████████  | 400/500 [00:27&lt;00:04, 22.48it/s]\neval model::  81%|████████  | 403/500 [00:28&lt;00:04, 23.03it/s]\neval model::  81%|████████  | 406/500 [00:28&lt;00:04, 21.86it/s]\neval model::  82%|████████▏ | 409/500 [00:28&lt;00:04, 21.82it/s]\neval model::  82%|████████▏ | 412/500 [00:28&lt;00:04, 21.81it/s]\neval model::  83%|████████▎ | 415/500 [00:28&lt;00:04, 17.89it/s]\neval model::  83%|████████▎ | 417/500 [00:28&lt;00:04, 18.09it/s]\neval model::  84%|████████▍ | 420/500 [00:28&lt;00:04, 19.54it/s]\neval model::  85%|████████▍ | 423/500 [00:29&lt;00:03, 21.34it/s]\neval model::  85%|████████▌ | 426/500 [00:29&lt;00:03, 21.98it/s]\neval model::  86%|████████▌ | 429/500 [00:29&lt;00:03, 21.10it/s]\neval model::  86%|████████▋ | 432/500 [00:29&lt;00:03, 20.41it/s]\neval model::  87%|████████▋ | 435/500 [00:29&lt;00:03, 20.86it/s]\neval model::  88%|████████▊ | 438/500 [00:29&lt;00:02, 21.49it/s]\neval model::  88%|████████▊ | 441/500 [00:30&lt;00:03, 17.85it/s]\neval model::  89%|████████▉ | 444/500 [00:30&lt;00:02, 19.56it/s]\neval model::  89%|████████▉ | 447/500 [00:30&lt;00:02, 19.42it/s]\neval model::  90%|█████████ | 450/500 [00:30&lt;00:02, 20.27it/s]\neval model::  91%|█████████ | 453/500 [00:30&lt;00:02, 21.03it/s]\neval model::  91%|█████████ | 456/500 [00:30&lt;00:01, 22.16it/s]\neval model::  92%|█████████▏| 459/500 [00:30&lt;00:01, 23.01it/s]\neval model::  92%|█████████▏| 462/500 [00:30&lt;00:01, 24.08it/s]\neval model::  93%|█████████▎| 465/500 [00:31&lt;00:01, 23.59it/s]\neval model::  94%|█████████▎| 468/500 [00:31&lt;00:01, 19.71it/s]\neval model::  94%|█████████▍| 471/500 [00:31&lt;00:01, 19.73it/s]\neval model::  95%|█████████▍| 474/500 [00:31&lt;00:01, 19.36it/s]\neval model::  95%|█████████▌| 477/500 [00:31&lt;00:01, 19.90it/s]\neval model::  96%|█████████▌| 480/500 [00:31&lt;00:00, 20.68it/s]\neval model::  97%|█████████▋| 483/500 [00:31&lt;00:00, 22.18it/s]\neval model::  97%|█████████▋| 486/500 [00:32&lt;00:00, 21.76it/s]\neval model::  98%|█████████▊| 489/500 [00:32&lt;00:00, 22.67it/s]\neval model::  98%|█████████▊| 492/500 [00:32&lt;00:00, 23.78it/s]\neval model::  99%|█████████▉| 495/500 [00:32&lt;00:00, 23.53it/s]\neval model:: 100%|█████████▉| 498/500 [00:32&lt;00:00, 23.97it/s]\neval model:: 100%|██████████| 500/500 [00:32&lt;00:00, 15.30it/s]\n[2024/11/03 04:51:45] ppocr INFO: cur metric, precision: 0.7611527832609554, recall: 0.6269918699186992, hmean: 0.6875891583452212, fps: 28.198555018217274\n[2024/11/03 04:51:45] ppocr INFO: save best model is to ./output/ch_PP-OCR_V3_det/best_accuracy\n[2024/11/03 04:51:45] ppocr INFO: best metric, hmean: 0.6875891583452212, is_float16: False, precision: 0.7611527832609554, recall: 0.6269918699186992, fps: 28.198555018217274, best_epoch: 7\n[2024/11/03 04:51:57] ppocr INFO: epoch: [7/9], global_step: 410, lr: 0.000053, loss: 1.931753, loss_shrink_maps: 1.127667, loss_threshold_maps: 0.598593, loss_binary_maps: 0.225462, loss_cbn: 0.000000, avg_reader_cost: 0.41823 s, avg_batch_cost: 1.00690 s, avg_samples: 8.0, ips: 7.94518 samples/s, eta: 0:06:03, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:52:29] ppocr INFO: epoch: [7/9], global_step: 420, lr: 0.000050, loss: 2.068067, loss_shrink_maps: 1.209053, loss_threshold_maps: 0.629119, loss_binary_maps: 0.242052, loss_cbn: 0.000000, avg_reader_cost: 1.99668 s, avg_batch_cost: 2.56330 s, avg_samples: 8.0, ips: 3.12097 samples/s, eta: 0:05:41, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:52:56] ppocr INFO: epoch: [7/9], global_step: 430, lr: 0.000047, loss: 2.313782, loss_shrink_maps: 1.401592, loss_threshold_maps: 0.638423, loss_binary_maps: 0.279927, loss_cbn: 0.000000, avg_reader_cost: 2.00502 s, avg_batch_cost: 2.58325 s, avg_samples: 8.0, ips: 3.09688 samples/s, eta: 0:05:18, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:53:17] ppocr INFO: epoch: [7/9], global_step: 440, lr: 0.000044, loss: 2.345259, loss_shrink_maps: 1.407669, loss_threshold_maps: 0.626240, loss_binary_maps: 0.280534, loss_cbn: 0.000000, avg_reader_cost: 1.43449 s, avg_batch_cost: 1.87595 s, avg_samples: 8.0, ips: 4.26451 samples/s, eta: 0:04:54, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:53:17] ppocr INFO: epoch: [7/9], global_step: 441, lr: 0.000044, loss: 2.245091, loss_shrink_maps: 1.351916, loss_threshold_maps: 0.618339, loss_binary_maps: 0.269428, loss_cbn: 0.000000, avg_reader_cost: 0.00019 s, avg_batch_cost: 0.01447 s, avg_samples: 0.4, ips: 27.64899 samples/s, eta: 0:04:51, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:53:18] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:53:49] ppocr INFO: epoch: [8/9], global_step: 450, lr: 0.000042, loss: 2.117996, loss_shrink_maps: 1.241391, loss_threshold_maps: 0.608490, loss_binary_maps: 0.248037, loss_cbn: 0.000000, avg_reader_cost: 1.62754 s, avg_batch_cost: 2.06135 s, avg_samples: 7.2, ips: 3.49285 samples/s, eta: 0:04:30, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:54:15] ppocr INFO: epoch: [8/9], global_step: 460, lr: 0.000039, loss: 2.087634, loss_shrink_maps: 1.223752, loss_threshold_maps: 0.607135, loss_binary_maps: 0.244480, loss_cbn: 0.000000, avg_reader_cost: 1.08461 s, avg_batch_cost: 1.61020 s, avg_samples: 8.0, ips: 4.96832 samples/s, eta: 0:04:05, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:54:47] ppocr INFO: epoch: [8/9], global_step: 470, lr: 0.000036, loss: 2.004470, loss_shrink_maps: 1.177044, loss_threshold_maps: 0.585685, loss_binary_maps: 0.235546, loss_cbn: 0.000000, avg_reader_cost: 2.05344 s, avg_batch_cost: 2.59522 s, avg_samples: 8.0, ips: 3.08260 samples/s, eta: 0:03:43, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:55:13] ppocr INFO: epoch: [8/9], global_step: 480, lr: 0.000034, loss: 1.992268, loss_shrink_maps: 1.166128, loss_threshold_maps: 0.586720, loss_binary_maps: 0.233275, loss_cbn: 0.000000, avg_reader_cost: 1.82055 s, avg_batch_cost: 2.40796 s, avg_samples: 8.0, ips: 3.32231 samples/s, eta: 0:03:20, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:55:48] ppocr INFO: epoch: [8/9], global_step: 490, lr: 0.000031, loss: 1.992268, loss_shrink_maps: 1.167522, loss_threshold_maps: 0.592154, loss_binary_maps: 0.233278, loss_cbn: 0.000000, avg_reader_cost: 1.58120 s, avg_batch_cost: 2.12634 s, avg_samples: 8.0, ips: 3.76234 samples/s, eta: 0:02:57, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:56:08] ppocr INFO: epoch: [8/9], global_step: 500, lr: 0.000029, loss: 2.146404, loss_shrink_maps: 1.249529, loss_threshold_maps: 0.646653, loss_binary_maps: 0.250221, loss_cbn: 0.000000, avg_reader_cost: 0.21264 s, avg_batch_cost: 0.73781 s, avg_samples: 8.0, ips: 10.84296 samples/s, eta: 0:02:32, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:56:12] ppocr INFO: epoch: [8/9], global_step: 504, lr: 0.000028, loss: 2.225155, loss_shrink_maps: 1.300825, loss_threshold_maps: 0.652690, loss_binary_maps: 0.260284, loss_cbn: 0.000000, avg_reader_cost: 0.14103 s, avg_batch_cost: 0.25246 s, avg_samples: 2.8, ips: 11.09088 samples/s, eta: 0:02:22, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:56:12] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:56:38] ppocr INFO: epoch: [9/9], global_step: 510, lr: 0.000026, loss: 2.225155, loss_shrink_maps: 1.300825, loss_threshold_maps: 0.659072, loss_binary_maps: 0.260284, loss_cbn: 0.000000, avg_reader_cost: 2.14283 s, avg_batch_cost: 2.53641 s, avg_samples: 4.8, ips: 1.89244 samples/s, eta: 0:02:09, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:57:02] ppocr INFO: epoch: [9/9], global_step: 520, lr: 0.000024, loss: 2.134806, loss_shrink_maps: 1.288284, loss_threshold_maps: 0.612116, loss_binary_maps: 0.257768, loss_cbn: 0.000000, avg_reader_cost: 1.67361 s, avg_batch_cost: 2.15418 s, avg_samples: 8.0, ips: 3.71371 samples/s, eta: 0:01:47, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:57:30] ppocr INFO: epoch: [9/9], global_step: 530, lr: 0.000021, loss: 1.980161, loss_shrink_maps: 1.148722, loss_threshold_maps: 0.590031, loss_binary_maps: 0.230329, loss_cbn: 0.000000, avg_reader_cost: 1.95521 s, avg_batch_cost: 2.57726 s, avg_samples: 8.0, ips: 3.10407 samples/s, eta: 0:01:24, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:58:03] ppocr INFO: epoch: [9/9], global_step: 540, lr: 0.000019, loss: 2.022417, loss_shrink_maps: 1.164846, loss_threshold_maps: 0.595614, loss_binary_maps: 0.233264, loss_cbn: 0.000000, avg_reader_cost: 2.37479 s, avg_batch_cost: 2.93655 s, avg_samples: 8.0, ips: 2.72428 samples/s, eta: 0:01:01, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:58:39] ppocr INFO: epoch: [9/9], global_step: 550, lr: 0.000017, loss: 2.125406, loss_shrink_maps: 1.259481, loss_threshold_maps: 0.639860, loss_binary_maps: 0.251433, loss_cbn: 0.000000, avg_reader_cost: 2.94293 s, avg_batch_cost: 3.43275 s, avg_samples: 8.0, ips: 2.33049 samples/s, eta: 0:00:39, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:58:58] ppocr INFO: epoch: [9/9], global_step: 560, lr: 0.000015, loss: 2.105257, loss_shrink_maps: 1.247652, loss_threshold_maps: 0.644058, loss_binary_maps: 0.249112, loss_cbn: 0.000000, avg_reader_cost: 1.12843 s, avg_batch_cost: 1.69826 s, avg_samples: 8.0, ips: 4.71070 samples/s, eta: 0:00:16, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:59:05] ppocr INFO: epoch: [9/9], global_step: 567, lr: 0.000014, loss: 2.059302, loss_shrink_maps: 1.213146, loss_threshold_maps: 0.593973, loss_binary_maps: 0.242463, loss_cbn: 0.000000, avg_reader_cost: 0.43328 s, avg_batch_cost: 0.67443 s, avg_samples: 5.2, ips: 7.71022 samples/s, eta: 0:00:00, max_mem_reserved: 6488 MB, max_mem_allocated: 5427 MB\n[2024/11/03 04:59:06] ppocr INFO: save model in ./output/ch_PP-OCR_V3_det/latest\n[2024/11/03 04:59:06] ppocr INFO: best metric, hmean: 0.6875891583452212, is_float16: False, precision: 0.7611527832609554, recall: 0.6269918699186992, fps: 28.198555018217274, best_epoch: 7\nI1103 04:59:06.580554   166 tcp_store.cc:273] receive shutdown event and so quit from MasterDaemon run loop\nLAUNCH INFO 2024-11-03 04:59:07,726 Pod completed\nLAUNCH INFO 2024-11-03 04:59:07,726 Exit code 0\n\n:::"
  },
  {
    "objectID": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#inference",
    "href": "posts/2025-01-16-paddleocr-texdet/fine-tuning-paddleocr-det-model.html#inference",
    "title": "Fine-tuning PaddleOCR’s text detection models",
    "section": "Inference",
    "text": "Inference\nExport the fine-tuned model to an inference format using PaddleOCR’s export script. The arguments specify: - The configuration file - The path to the pretrained model weights. We’re going to use the model best_accuracy.pdparams that was generated in the directory we specify in config[\"Global\"][\"save_model_dir\"]. - The output directory for the saved inference model.\n\n!python3 /kaggle/working/PaddleOCR/tools/export_model.py -c /kaggle/working/config.yml -o Global.pretrained_model=\"/kaggle/working/output/ch_PP-OCR_V3_det/best_accuracy.pdparams\" Global.save_inference_dir=\"/kaggle/working/output/det-db-inference/\"\n\nW1103 04:59:12.464147   783 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.4, Runtime API Version: 11.8\nW1103 04:59:12.465219   783 gpu_resources.cc:149] device: 0, cuDNN Version: 90.0.\n[2024/11/03 04:59:12] ppocr INFO: load pretrain successful from /kaggle/working/output/ch_PP-OCR_V3_det/best_accuracy\nI1103 04:59:16.345665   783 interpretercore.cc:237] New Executor is Running.\n[2024/11/03 04:59:16] ppocr INFO: inference model is saved to /kaggle/working/output/det-db-inference/inference\n[2024/11/03 04:59:16] ppocr INFO: Export inference config file to /kaggle/working/output/det-db-inference/inference.yml\n\n\nLet’s make inference with some images with curved text.\n\n!python3 /kaggle/working/PaddleOCR/tools/infer/predict_det.py --det_algorithm=\"DB\" --det_model_dir=\"/kaggle/working/output/det-db-inference\" --image_dir=\"/kaggle/input/examples-for-fine-tuned-paddleocr-on-scutctw1500\" --use_gpu=True\n\n[2024/11/03 04:59:22] ppocr WARNING: The first GPU is used for inference by default, GPU ID: 0\n[2024/11/03 04:59:23] ppocr INFO: 1oAxJ.jpg [[[1379.0, 2810.0], [1589.0, 2810.0], [1589.0, 2906.0], [1379.0, 2906.0]], [[1657.0, 2986.0], [2021.0, 2717.0], [2128.0, 2855.0], [1765.0, 3125.0]], [[804.0, 2516.0], [911.0, 2499.0], [936.0, 2654.0], [829.0, 2671.0]], [[2070.0, 2470.0], [2195.0, 2470.0], [2195.0, 2617.0], [2070.0, 2617.0]], [[1922.0, 2383.0], [2020.0, 2404.0], [1974.0, 2615.0], [1876.0, 2594.0]], [[997.0, 2205.0], [1121.0, 2205.0], [1121.0, 2633.0], [997.0, 2633.0]], [[1817.0, 2058.0], [1881.0, 2058.0], [1881.0, 2117.0], [1817.0, 2117.0]], [[1864.0, 1882.0], [2058.0, 1882.0], [2058.0, 2079.0], [1864.0, 2079.0]], [[851.0, 2037.0], [1082.0, 1810.0], [1211.0, 1936.0], [979.0, 2163.0]]]\n\n[2024/11/03 04:59:23] ppocr INFO: 0 The predict time of /kaggle/input/examples-for-fine-tuned-paddleocr-on-scutctw1500/1oAxJ.jpg: 0.5031492710113525\n[2024/11/03 04:59:23] ppocr INFO: The visualized image saved in ./inference_results/det_res_1oAxJ.jpg\n[2024/11/03 04:59:23] ppocr INFO: Captura de pantalla 2024-09-09 114728.png [[[58.0, 276.0], [280.0, 276.0], [280.0, 299.0], [58.0, 299.0]], [[93.0, 250.0], [244.0, 250.0], [244.0, 273.0], [93.0, 273.0]], [[56.0, 89.0], [89.0, 92.0], [80.0, 173.0], [47.0, 170.0]], [[246.0, 93.0], [270.0, 87.0], [280.0, 122.0], [256.0, 128.0]], [[69.0, 78.0], [88.0, 78.0], [88.0, 100.0], [69.0, 100.0]], [[243.0, 73.0], [256.0, 73.0], [256.0, 87.0], [243.0, 87.0]], [[167.0, 24.0], [199.0, 24.0], [199.0, 44.0], [167.0, 44.0]]]\n\n[2024/11/03 04:59:23] ppocr INFO: 1 The predict time of /kaggle/input/examples-for-fine-tuned-paddleocr-on-scutctw1500/Captura de pantalla 2024-09-09 114728.png: 0.04157447814941406\n[2024/11/03 04:59:23] ppocr INFO: The visualized image saved in ./inference_results/det_res_Captura de pantalla 2024-09-09 114728.png\n[2024/11/03 04:59:23] ppocr INFO: Captura de pantalla 2024-09-09 115835.png [[[155.0, 53.0], [255.0, 64.0], [252.0, 89.0], [152.0, 77.0]], [[72.0, 66.0], [162.0, 54.0], [165.0, 76.0], [75.0, 88.0]]]\n\n[2024/11/03 04:59:23] ppocr INFO: 2 The predict time of /kaggle/input/examples-for-fine-tuned-paddleocr-on-scutctw1500/Captura de pantalla 2024-09-09 115835.png: 0.015452146530151367\n[2024/11/03 04:59:23] ppocr INFO: The visualized image saved in ./inference_results/det_res_Captura de pantalla 2024-09-09 115835.png\n[2024/11/03 04:59:23] ppocr INFO: Captura de pantalla 2024-09-11 141037.png [[[39.0, 270.0], [233.0, 270.0], [233.0, 291.0], [39.0, 291.0]], [[38.0, 243.0], [237.0, 243.0], [237.0, 261.0], [38.0, 261.0]], [[7.0, 216.0], [261.0, 216.0], [261.0, 233.0], [7.0, 233.0]]]\n\n[2024/11/03 04:59:23] ppocr INFO: 3 The predict time of /kaggle/input/examples-for-fine-tuned-paddleocr-on-scutctw1500/Captura de pantalla 2024-09-11 141037.png: 0.027079105377197266\n[2024/11/03 04:59:23] ppocr INFO: The visualized image saved in ./inference_results/det_res_Captura de pantalla 2024-09-11 141037.png\n[2024/11/03 04:59:24] ppocr INFO: circular-text-photography-script-logo-design-template-034256fa3021b860a295f4f66c4d321b_screen.jpg [[[164.0, 302.0], [526.0, 302.0], [526.0, 353.0], [164.0, 353.0]]]\n\n[2024/11/03 04:59:24] ppocr INFO: 4 The predict time of /kaggle/input/examples-for-fine-tuned-paddleocr-on-scutctw1500/circular-text-photography-script-logo-design-template-034256fa3021b860a295f4f66c4d321b_screen.jpg: 0.05555558204650879\n[2024/11/03 04:59:24] ppocr INFO: The visualized image saved in ./inference_results/det_res_circular-text-photography-script-logo-design-template-034256fa3021b860a295f4f66c4d321b_screen.jpg\n\n\n\nfrom PIL import Image\nimage = Image.open(\"/kaggle/working/inference_results/det_res_Captura de pantalla 2024-09-09 114728.png\")\nimage\n\n\n\n\n\n\n\n\n\nImage.open(\"/kaggle/working/inference_results/det_res_circular-text-photography-script-logo-design-template-034256fa3021b860a295f4f66c4d321b_screen.jpg\")\n\n\n\n\n\n\n\n\n\nImage.open(\"/kaggle/working/inference_results/det_res_Captura de pantalla 2024-09-11 141037.png\")\n\n\n\n\n\n\n\n\n\nImage.open(\"/kaggle/working/inference_results/det_res_Captura de pantalla 2024-09-09 115835.png\")\n\n\n\n\n\n\n\n\n\nImage.open(\"/kaggle/working/inference_results/det_res_1oAxJ.jpg\")\n\n\n\n\n\n\n\n\nAs we can see in the inference examples, our model struggles with curved text, despite being fine-tuned on a dataset with this type of text. Nonetheless, the metrics were not so bad, there is room for improvement, though, with hmean close to 0.70, precision near to 0.78 and recall around 0.62. The reason for these decent metrics is that not all text instances in the images are curved; there are also instances of text in the conventional horizontal layout, where the model performs well.\nAll in all, it was a good warm up to learn how to fine-tune the PaddleOCR’s detection models. The next time we will try an algorithm specialized in curved text like SAST.\nNothing would make me happier than knowing that my notebook has been helpful to you. If it has, please let me know with a like on the notebook! ❤️"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]